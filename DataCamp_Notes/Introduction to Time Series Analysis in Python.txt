Course Description
From stock prices to climate data, time series data are found in a wide variety of domains, and being able to effectively work with such data is an increasingly important skill for data scientists. This course will introduce you to time series analysis in Python. After learning about what a time series is, you'll learn about several time series models ranging from autoregressive and moving average models to cointegration models. Along the way, you'll learn how to estimate, forecast, and simulate these models using statistical libraries in Python. You'll see numerous examples of how these models are used, with a particular emphasis on applications in finance.

<=====================================================================================================================================>

1
Correlation and Autocorrelation
FREE
0%
In this chapter you'll be introduced to the ideas of correlation and autocorrelation for time series. Correlation describes the relationship between two time series and autocorrelation describes the relationship of a time series with its past values.

_____________________________________________________________________________________________________________________________________

A "Thin" Application of Time Series
Google Trends allows users to see how often a term is searched for. We downloaded a file from Google Trends containing the frequency over time for the search word "diet", which is pre-loaded in a DataFrame called diet. A first step when analyzing a time series is to visualize the data with a plot. You should be able to clearly see a gradual decrease in searches for "diet" throughout the calendar year, hitting a low around the December holidays, followed by a spike in searches around the new year as people make New Year's resolutions to lose weight.

Like many time series datasets you will be working with, the index of dates are strings and should be converted to a datetime index before plotting.

This course touches on a lot of concepts you may have forgotten, so if you ever need a quick refresher, download the pandas basics Cheat Sheet and keep it handy!

Instructions 1/3
50 XP
1
2
3
Convert the date index to datetime using pandas's to_datetime()


# Import pandas and plotting modules
import pandas as pd
import matplotlib.pyplot as plt

# Convert the date index to datetime
diet.index = pd.to_datetime(diet.index)


Plot the time series and set the argument grid to True to better see the year-ends.

# From previous step
diet.index = pd.to_datetime(diet.index)

# Plot the entire time series diet and show gridlines
diet.plot(grid=True)
plt.show()


Slice the diet dataset to keep only values from 2012, assigning to diet2012.
Plot the diet2012, again creating gridlines with the grid argument.


# From previous step
diet.index = pd.to_datetime(diet.index)

# Slice the dataset to keep only 2012
diet2012 = diet['2012']

# Plot 2012 data
diet2012.plot(grid=True)
plt.show()
_____________________________________________________________________________________________________________________________________


Merging Time Series With Different Dates
Stock and bond markets in the U.S. are closed on different days. For example, although the bond market is closed on Columbus Day (around Oct 12) and Veterans Day (around Nov 11), the stock market is open on those days. One way to see the dates that the stock market is open and the bond market is closed is to convert both indexes of dates into sets and take the difference in sets.

The pandas .join() method is a convenient tool to merge the stock and bond DataFrames on dates when both markets are open.

Stock prices and 10-year US Government bond yields, which were downloaded from FRED, are pre-loaded in DataFrames stocks and bonds.

Instructions
100 XP
Convert the dates in the stocks.index and bonds.index into sets.
Take the difference of the stock set minus the bond set to get those dates where the stock market has data but the bond market does not.
Merge the two DataFrames into a new DataFrame, stocks_and_bonds using the .join() method, which has the syntax df1.join(df2).
To get the intersection of dates, use the argument how='inner'.


# Import pandas
import pandas as pd

# Convert the stock index and bond index into sets
set_stock_dates = set(stocks.index)
set_bond_dates = set(bonds.index)

# Take the difference between the sets and print
print(set_stock_dates - set_bond_dates)

# Merge stocks and bonds DataFrames using join()
stocks_and_bonds = stocks.join(bonds, how='inner')

<script.py> output:
    {'2009-11-11', '2007-11-12', '2015-10-12', '2011-10-10', '2010-11-11', '2012-11-12', '2010-10-11', '2015-11-11', '2007-10-08', '2014-10-13', '2008-11-11', '2016-10-10', '2009-10-12', '2017-06-09', '2011-11-11', '2008-10-13', '2014-11-11', '2013-11-11', '2016-11-11', '2013-10-14', '2012-10-08'}
	_____________________________________________________________________________________________________________________________________
	
	Correlation of Stocks and Bonds
Investors are often interested in the correlation between the returns of two different assets for asset allocation and hedging purposes. In this exercise, you'll try to answer the question of whether stocks are positively or negatively correlated with bonds. Scatter plots are also useful for visualizing the correlation between the two variables.

Keep in mind that you should compute the correlations on the percentage changes rather than the levels.

Stock prices and 10-year bond yields are combined in a DataFrame called stocks_and_bonds under columns SP500 and US10Y

The pandas and plotting modules have already been imported for you. For the remainder of the course, pandas is imported as pd and matplotlib.pyplot is imported as plt.

Instructions
100 XP
Instructions
100 XP
Compute percent changes on the stocks_and_bonds DataFrame using the .pct_change() method and call the new DataFrame returns.
Compute the correlation of the columns SP500 and US10Y in the returns DataFrame using the .corr() method for Series which has the syntax series1.corr(series2).
Show a scatter plot of the percentage change in stock and bond yields.


# Compute percent change using pct_change()
returns = stocks_and_bonds.pct_change()

# Compute correlation using corr()
correlation = returns['SP500'].corr(returns['US10Y'])
print("Correlation of stocks and interest rates: ", correlation)

# Make scatter plot
plt.scatter(x=returns.SP500, y=returns.US10Y)
plt.show()

	
<script.py> output:
    Correlation of stocks and interest rates:  0.4119448886249272
		
	The positive correlation means that when interest rates go down, stock prices go down. For example, during crises like 9/11, investors sold stocks and moved their money to less risky bonds (this is sometimes referred to as a 'flight to quality'). During these periods, stocks drop and interest rates drop as well. Of course, there are times when the opposite relationship holds too.	
		
_____________________________________________________________________________________________________________________________________

Flying Saucers Aren't Correlated to Flying Markets
Two trending series may show a strong correlation even if they are completely unrelated. This is referred to as "spurious correlation". That's why when you look at the correlation of say, two stocks, you should look at the correlation of their returns and not their levels.

To illustrate this point, calculate the correlation between the levels of the stock market and the annual sightings of UFOs. Both of those time series have trended up over the last several decades, and the correlation of their levels is very high. Then calculate the correlation of their percent changes. This will be close to zero, since there is no relationship between those two series.

The DataFrame levels contains the levels of DJI and UFO. UFO data was downloaded from www.nuforc.org.

Instructions
100 XP
Calculate the correlation of the columns DJI and UFO.
Create a new DataFrame of changes using the .pct_change() method.
Re-calculate the correlation of the columns DJI and UFO on the changes.

# Compute correlation of levels
correlation1 = levels['DJI'].corr(levels['UFO'])
print("Correlation of levels: ", correlation1)

# Compute correlation of percent changes
changes = levels.pct_change()
correlation2 = changes['DJI'].corr(changes['UFO'])
print("Correlation of changes: ", correlation2)

<script.py> output:
    Correlation of levels:  0.9399762210726432
    Correlation of changes:  0.06026935462405376

Great work! Notice that the correlation on levels is high but the correlation on changes is close to zero.
_____________________________________________________________________________________________________________________________________


Looking at a Regression's R-Squared
R-squared measures how closely the data fit the regression line, so the R-squared in a simple regression is related to the correlation between the two variables. In particular, the magnitude of the correlation is the square root of the R-squared and the sign of the correlation is the sign of the regression coefficient.

In this exercise, you will start using the statistical package statsmodels, which performs much of the statistical modeling and testing that is found in R and software packages like SAS and MATLAB.

You will take two series, x and y, compute their correlation, and then regress y on x using the function OLS(y,x) in the statsmodels.api library (note that the dependent, or right-hand side variable y is the first argument). Most linear regressions contain a constant term which is the intercept (the α in the regression yt=α+βxt+ϵt). To include a constant using the function OLS(), you need to add a column of 1's to the right hand side of the regression.

The module statsmodels.api has been imported for you as sm.

Instructions
100 XP
Instructions
100 XP
Compute the correlation between x and y using the .corr() method.
Run a regression:
First convert the Series x to a DataFrame dfx.
Add a constant using sm.add_constant(), assigning it to dfx1
Regress y on dfx1 using sm.OLS().fit().
Print out the results of the regression and compare the R-squared with the correlation.

# Import the statsmodels module
import statsmodels.api as sm

# Compute correlation of x and y
correlation = x.corr(y)
print("The correlation between x and y is %4.2f" %(correlation))

# Convert the Series x to a DataFrame and name the column x
dfx = pd.DataFrame(x, columns=['x'])

# Add a constant to the DataFrame dfx
dfx1 = sm.add_constant(dfx)

# Regress y on dfx1
result = sm.OLS(y, dfx1).fit()

# Print out the results and look at the relationship between R-squared and the correlation above
print(result.summary())


<script.py> output:
    The correlation between x and y is -0.90
                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:                      y   R-squared:                       0.818
    Model:                            OLS   Adj. R-squared:                  0.817
    Method:                 Least Squares   F-statistic:                     4471.
    Date:                Sun, 06 Oct 2019   Prob (F-statistic):               0.00
    Time:                        22:38:46   Log-Likelihood:                -560.10
    No. Observations:                1000   AIC:                             1124.
    Df Residuals:                     998   BIC:                             1134.
    Df Model:                           1                                         
    Covariance Type:            nonrobust                                         
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    const         -0.0052      0.013     -0.391      0.696      -0.032       0.021
    x             -0.9080      0.014    -66.869      0.000      -0.935      -0.881
    ==============================================================================
    Omnibus:                        0.048   Durbin-Watson:                   2.066
    Prob(Omnibus):                  0.976   Jarque-Bera (JB):                0.103
    Skew:                          -0.003   Prob(JB):                        0.950
    Kurtosis:                       2.951   Cond. No.                         1.03
    ==============================================================================
    
    Warnings:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.


 +100 XP
Notice that the two different methods of computing correlation give the same result. The correlation is about -0.9 and the R-squared is about 0.81

_____________________________________________________________________________________________________________________________________

A Popular Strategy Using Autocorrelation
One puzzling anomaly with stocks is that investors tend to overreact to news. Following large jumps, either up or down, stock prices tend to reverse. This is described as mean reversion in stock prices: prices tend to bounce back, or revert, towards previous levels after large moves, which are observed over time horizons of about a week. A more mathematical way to describe mean reversion is to say that stock returns are negatively autocorrelated.

This simple idea is actually the basis for a popular hedge fund strategy. If you're curious to learn more about this hedge fund strategy (although it's not necessary reading for anything else later in the course), see here.

You'll look at the autocorrelation of weekly returns of MSFT stock from 2012 to 2017. You'll start with a DataFrame MSFT of daily prices. You should use the .resample() method to get weekly prices and then compute returns from prices. Use the pandas method .autocorr() to get the autocorrelation and show that the autocorrelation is negative. Note that the .autocorr() method only works on Series, not DataFrames (even DataFrames with one column), so you will have to select the column in the DataFrame.

Instructions
100 XP
Use the .resample() method with rule='W' and how='last'to convert daily data to weekly data.
The argument how in .resample() has been deprecated.
The new syntax .resample().last() also works.
Create a new DataFrame, returns, of percent changes in weekly prices using the .pct_change() method.
Compute the autocorrelation using the .autocorr() method on the series of closing stock prices, which is the column 'Adj Close' in the DataFrame returns.

# Convert the daily data to weekly data
MSFT = MSFT.resample(rule='W',how='last')

# Compute the percentage change of prices
returns = MSFT.pct_change()

# Compute and print the autocorrelation of returns
autocorrelation = returns['Adj Close'].autocorr()
print("The autocorrelation of weekly returns is %4.2f" %(autocorrelation))


<script.py> output:
    The autocorrelation of weekly returns is -0.16
    

_____________________________________________________________________________________________________________________________________

Are Interest Rates Autocorrelated?
When you look at daily changes in interest rates, the autocorrelation is close to zero. However, if you resample the data and look at annual changes, the autocorrelation is negative. This implies that while short term changes in interest rates may be uncorrelated, long term changes in interest rates are negatively autocorrelated. A daily move up or down in interest rates is unlikely to tell you anything about interest rates tomorrow, but a move in interest rates over a year can tell you something about where interest rates are going over the next year. And this makes some economic sense: over long horizons, when interest rates go up, the economy tends to slow down, which consequently causes interest rates to fall, and vice versa.

The DataFrame daily_rates contains daily data of 10-year interest rates from 1962 to 2017.

Instructions
100 XP
Create a new DataFrame, daily_diff, of changes in daily rates using the .diff() method.
Compute the autocorrelation of the column 'US10Y' in daily_diff using the .autocorr() method.
Use the .resample() method with arguments rule='A' to convert to annual frequency and how='last'.
The argument how in .resample() has been deprecated.
The new syntax .resample().last() also works.
Create a new DataFrame, yearly_diff of changes in annual rates and compute the autocorrelation, as above.

# Compute the daily change in interest rates
daily_diff = daily_rates.diff()

# Compute and print the autocorrelation of daily changes
autocorrelation_daily = daily_diff['US10Y'].autocorr()
print("The autocorrelation of daily interest rate changes is %4.2f" %(autocorrelation_daily))

# Convert the daily data to annual data
yearly_rates = daily_rates.resample(rule='A').last()

# Repeat above for annual data
yearly_diff = yearly_rates.diff()
autocorrelation_yearly = yearly_diff['US10Y'].autocorr()
print("The autocorrelation of annual interest rate changes is %4.2f" %(autocorrelation_yearly))


<script.py> output:
    The autocorrelation of daily interest rate changes is 0.07
    The autocorrelation of annual interest rate changes is -0.22
    
<=====================================================================================================================================>


2
Some Simple Time Series
0%
In this chapter you'll learn about some simple time series models. These include white noise and a random walk.
_____________________________________________________________________________________________________________________________________

Taxing Exercise: Compute the ACF
In the last chapter, you computed autocorrelations with one lag. Often we are interested in seeing the autocorrelation over many lags. The quarterly earnings for H&R Block (ticker symbol HRB) is plotted on the right, and you can see the extreme cyclicality of its earnings. A vast majority of its earnings occurs in the quarter that taxes are due.

You will compute the array of autocorrelations for the H&R Block quarterly earnings that is pre-loaded in the DataFrame HRB. Then, plot the autocorrelation function using the plot_acf module. This plot shows what the autocorrelation function looks like for cyclical earnings data. The ACF at lag=0 is always one, of course. In the next exercise, you will learn about the confidence interval for the ACF, but for now, suppress the confidence interval by setting alpha=1.

Instructions
100 XP
Import the acf module and plot_acf module from statsmodels.
Compute the array of autocorrelations of the quarterly earnings data in DataFrame HRB.
Plot the autocorrelation function of the quarterly earnings data in HRB, and pass the argument alpha=1 to suppress the confidence interval.


# Import the acf module and the plot_acf module from statsmodels
from statsmodels.tsa.stattools import acf
from statsmodels.graphics.tsaplots import plot_acf

# Compute the acf array of HRB
acf_array = acf(HRB)
print(acf_array)

# Plot the acf function
plot_acf(HRB, alpha=1)
plt.show()


<script.py> output:
    [ 1.         -0.22122696 -0.39856504 -0.26615093  0.83479804 -0.1901038
     -0.3475634  -0.23140368  0.71995993 -0.15661007 -0.29766783 -0.22097189
      0.61656933 -0.15022869 -0.27922022 -0.22465946  0.5725259  -0.08758288
     -0.24075584 -0.20363054  0.4797058  -0.06091139 -0.20935484 -0.18303202
      0.42481275 -0.03352559 -0.17471087 -0.16384328  0.34341079 -0.01734364
     -0.13820811 -0.12232172  0.28407164 -0.01927656 -0.11757974 -0.10386933
      0.20156485 -0.0120634  -0.07509539 -0.0707104   0.10222029]
      
      
 _____________________________________________________________________________________________________________________________________
 
 
Are We Confident This Stock is Mean Reverting?
In the last chapter, you saw that the autocorrelation of MSFT's weekly stock returns was -0.16. That autocorrelation seems large, but is it statistically significant? In other words, can you say that there is less than a 5% chance that we would observe such a large negative autocorrelation if the true autocorrelation were really zero? And are there any autocorrelations at other lags that are significantly different from zero?

Even if the true autocorrelations were zero at all lags, in a finite sample of returns you won't see the estimate of the autocorrelations exactly zero. In fact, the standard deviation of the sample autocorrelation is 1/N−−√ where N is the number of observations, so if N=100, for example, the standard deviation of the ACF is 0.1, and since 95% of a normal curve is between +1.96 and -1.96 standard deviations from the mean, the 95% confidence interval is ±1.96/N−−√. This approximation only holds when the true autocorrelations are all zero.

You will compute the actual and approximate confidence interval for the ACF, and compare it to the lag-one autocorrelation of -0.16 from the last chapter. The weekly returns of Microsoft is pre-loaded in a DataFrame called returns.

Instructions
100 XP
Instructions
100 XP
Recompute the autocorrelation of weekly returns in the Series 'Adj Close' in the returns DataFrame.
Find the number of observations in the returns DataFrame using the len() function.
Approximate the 95% confidence interval of the estimated autocorrelation. The math function sqrt() has been imported and can be used.
Plot the autocorrelation function of returns using plot_acf that was imported from statsmodels. Set alpha=0.05 for the confidence intervals (that's the default) and lags=20.

# Import the plot_acf module from statsmodels and sqrt from math
from statsmodels.graphics.tsaplots import plot_acf
from math import sqrt

# Compute and print the autocorrelation of MSFT weekly returns
autocorrelation = returns['Adj Close'].autocorr()
print("The autocorrelation of weekly MSFT returns is %4.2f" %(autocorrelation))

# Find the number of observations by taking the length of the returns DataFrame
nobs = len(returns)

# Compute the approximate confidence interval
conf = 1.96/sqrt(nobs)
print("The approximate confidence interval is +/- %4.2f" %(conf))

# Plot the autocorrelation function with 95% confidence intervals and 20 lags using plot_acf
plot_acf(returns, alpha=0.05, lags=20)
plt.show()


<script.py> output:
    The autocorrelation of weekly MSFT returns is -0.16
    The approximate confidence interval is +/- 0.12
    
    
Notice that the autocorrelation with lag 1 is significantly negative, but none of the other lags are significantly different from zero

  _____________________________________________________________________________________________________________________________________
  
  
  Can't Forecast White Noise
A white noise time series is simply a sequence of uncorrelated random variables that are identically distributed. Stock returns are often modeled as white noise. Unfortunately, for white noise, we cannot forecast future observations based on the past - autocorrelations at all lags are zero.

You will generate a white noise series and plot the autocorrelation function to show that it is zero for all lags. You can use np.random.normal() to generate random returns. For a Gaussian white noise process, the mean and standard deviation describe the entire process.

Plot this white noise series to see what it looks like, and then plot the autocorrelation function.

Instructions
100 XP
Generate 1000 random normal returns using np.random.normal() with mean 2% (0.02) and standard deviation 5% (0.05), where the argument for the mean is loc and the argument for the standard deviation is scale.
Verify the mean and standard deviation of returns using np.mean() and np.std().
Plot the time series.
Plot the autocorrelation function using plot_acf with lags=20.

# Import the plot_acf module from statsmodels
from statsmodels.graphics.tsaplots import plot_acf

# Simulate white noise returns
returns = np.random.normal(loc=0.02, scale=0.05, size=1000)

# Print out the mean and standard deviation of returns
mean = np.mean(returns)
std = np.std(returns)
print("The mean is %5.3f and the standard deviation is %5.3f" %(mean,std))

# Plot returns series
plt.plot(returns)
plt.show()

# Plot autocorrelation function of white noise returns
plot_acf(returns, lags=20)
plt.show()

<script.py> output:
    The mean is 0.018 and the standard deviation is 0.050


Notice that for a white noise time series, all the autocorrelations are close to zero, so the past will not help you forecast the future.

_____________________________________________________________________________________________________________________________________
Generate a Random Walk
Whereas stock returns are often modeled as white noise, stock prices closely follow a random walk. In other words, today's price is yesterday's price plus some random noise.

You will simulate the price of a stock over time that has a starting price of 100 and every day goes up or down by a random amount. Then, plot the simulated stock price. If you hit the "Run Code" code button multiple times, you'll see several realizations.

Instructions
100 XP
Generate 500 random normal "steps" with mean=0 and standard deviation=1 using np.random.normal(), where the argument for the mean is loc and the argument for the standard deviation is scale.
Simulate stock prices P:
Cumulate the random steps using the numpy .cumsum() method
Add 100 to P to get a starting stock price of 100.
Plot the simulated random walk


# Generate 500 random steps with mean=0 and standard deviation=1
steps = np.random.normal(loc=0, scale=1, size=500)

# Set first element to 0 so that the first price will be the starting stock price
steps[0]=0

# Simulate stock prices, P with a starting price of 100
P = 100 + np.cumsum(steps)

# Plot the simulated stock prices
plt.plot(P)
plt.title("Simulated Random Walk")
plt.show()

The simulated price series you plotted should closely resemble a random walk.
_____________________________________________________________________________________________________________________________________

Get the Drift
In the last exercise, you simulated stock prices that follow a random walk. You will extend this in two ways in this exercise.

You will look at a random walk with a drift. Many time series, like stock prices, are random walks but tend to drift up over time.
In the last exercise, the noise in the random walk was additive: random, normal changes in price were added to the last price. However, when adding noise, you could theoretically get negative prices. Now you will make the noise multiplicative: you will add one to the random, normal changes to get a total return, and multiply that by the last price.
Instructions
100 XP
Generate 500 random normal multiplicative "steps" with mean 0.1% and standard deviation 1% using np.random.normal(), which are now returns, and add one for total return.
Simulate stock prices P:
Cumulate the product of the steps using the numpy .cumprod() method.
Multiply the cumulative product of total returns by 100 to get a starting value of 100.
Plot the simulated random walk with drift.

# Generate 500 random steps
steps = np.random.normal(loc=0.001, scale=0.01, size=500) + 1

# Set first element to 1
steps[0]=1

# Simulate the stock price, P, by taking the cumulative product
P = 100 * np.cumprod(steps)

# Plot the simulated stock prices
plt.plot(P)
plt.title("Simulated Random Walk with Drift")
plt.show()

This simulated price series you plotted should closely resemble a random walk for a high flying stock

_____________________________________________________________________________________________________________________________________

Are Stock Prices a Random Walk?
Most stock prices follow a random walk (perhaps with a drift). You will look at a time series of Amazon stock prices, pre-loaded in the DataFrame AMZN, and run the 'Augmented Dickey-Fuller Test' from the statsmodels library to show that it does indeed follow a random walk.

With the ADF test, the "null hypothesis" (the hypothesis that we either reject or fail to reject) is that the series follows a random walk. Therefore, a low p-value (say less than 5%) means we can reject the null hypothesis that the series is a random walk.

Instructions
100 XP
Import the adfuller module from statsmodels.
Run the Augmented Dickey-Fuller test on the series of closing stock prices, which is the column 'Adj Close' in the AMZN DataFrame.
Print out the entire output, which includes the test statistic, the p-values, and the critical values for tests with 1%, 10%, and 5% levels.
Print out just the p-value of the test (results[0] is the test statistic, and results[1] is the p-value).

# Import the adfuller module from statsmodels
from statsmodels.tsa.stattools import adfuller

# Run the ADF test on the price series and print out the results
results = adfuller(AMZN['Adj Close'])
print(results)

# Just print out the p-value
print('The p-value of the test on prices is: ' + str(results[1]))

<script.py> output:
    (4.02516852577074, 1.0, 33, 5054, {'1%': -3.4316445438146865, '5%': -2.862112049726916, '10%': -2.5670745025321304}, 30308.64216426981)
    The p-value of the test on prices is: 1.0

According to this test, we cannot reject the hypothesis that Amazon prices follow a random walk. In the next exercise, you'll look at Amazon returns.

_____________________________________________________________________________________________________________________________________

How About Stock Returns?
In the last exercise, you showed that Amazon stock prices, contained in the DataFrame AMZN follow a random walk. In this exercise. you will do the same thing for Amazon returns (percent change in prices) and show that the returns do not follow a random walk.

Instructions
100 XP
Import the adfuller module from statsmodels.
Create a new DataFrame of AMZN returns by taking the percent change of prices using the method .pct_change().
Eliminate the NaN in the first row of returns using the .dropna() method on the DataFrame.
Run the Augmented Dickey-Fuller test on the 'Adj Close' column of AMZN_ret, and print out the p-value in results[1].


# Import the adfuller module from statsmodels
from statsmodels.tsa.stattools import adfuller

# Create a DataFrame of AMZN returns
AMZN_ret = AMZN.pct_change()

# Eliminate the NaN in the first row of returns
AMZN_ret = AMZN_ret.dropna()

# Run the ADF test on the return series and print out the p-value
results = adfuller(AMZN_ret['Adj Close'])
print('The p-value of the test on returns is: ' + str(results[1]))




 +100 XP
The p-value is extremely small, so we can easily reject the hypothesis that returns are a random walk at all levels of significance.
_____________________________________________________________________________________________________________________________________




<=====================================================================================================================================>

3
Autoregressive (AR) Models
0%
In this chapter you'll learn about autoregressive, or AR, models for time series. These models use past values of the series to predict the current value.

<=====================================================================================================================================>

4
Moving Average (MA) and ARMA Models
0%
In this chapter you'll learn about another kind of model, the moving average, or MA, model. You will also see how to combine AR and MA models into a powerful ARMA model.

VIEW CHAPTER DETAILS
<=====================================================================================================================================>

5
Putting It All Together
0%
This chapter will show you how to model two series jointly using cointegration models. Then you'll wrap up with a case study where you look at a time series of temperature data from New York City.
