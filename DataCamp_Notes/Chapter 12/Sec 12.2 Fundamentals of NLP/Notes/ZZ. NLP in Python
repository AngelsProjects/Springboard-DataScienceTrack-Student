Natural Language Processing Fundamentals in Python

Course Description
In this course, you'll learn Natural Language Processing (NLP) basics, such as how to identify and separate words, how to extract topics in a text, and how to build your own fake news classifier. You'll also learn how to use basic libraries such as NLTK, alongside libraries which utilize deep learning to solve common NLP problems. This course will give you the foundation to process and parse text as you move forward in your Python learning.

===============================================================================================================================

1
Regular expressions & word tokenization
FREE
0%
This chapter will introduce some basic NLP concepts, such as word tokenization and regular expressions to help parse text. You'll also learn how to handle non-English text and more difficult tokenization you might find as you explore the wide world of NLP.

_____________________________________________________________________________________________________________________________

Introduction to regular
expressions

What is Natural Language Processing?
Field of study focused on making sense of language
Using statistics and computers
You will learn the basics of NLP
Topic identification
Text classification
NLP applications include:
Chatbots
Translation
Sentiment analysis
... and many more!



What exactly are regular expressions?
Strings with a special syntax
Allow us to match patterns in other strings
Applications of regular expressions:
Find all web links in a document
Parse email addresses, remove/replace unwanted characters

In [1]: import re
In [2]: re.match('abc'
,
'abcdef')
Out[2]: <_sre.SRE_Match object; span=(0, 3), match='abc'>
In [3]: word_regex = '\w+'
In [4]: re.match(word_regex,
'hi there!')
Out[4]: <_sre.SRE_Match object; span=(0, 2), match='hi'>


Common regex patterns (7)
pattern matches example
\w+ word 'Magic'
\d digit 9
\s space ' '
.* wildcard 'username74'
+ or * greedy match 'aaaaaa'
\S not space 'no_spaces'
[a-z] lowercase group 'abcdefg




Python's re Module
re module
split: split a string on regex
findall: find all patterns in a string
search: search for a pattern
match: match an entire string or substring based on a pattern
Pattern first, and the string second
May return an iterator, string, or match object
In [5]: re.split('\s+'
,
'Split on spaces.')
Out[5]: ['Split'
,
'on'
,
'spaces.']

________________________________________________________________________________________________________________________

Practicing regular expressions: re.split() and re.findall()
Now you'll get a chance to write some regular expressions to match digits, strings and non-alphanumeric characters. Take a look at my_string first by printing it in the IPython Shell, to determine how you might best match the different steps.

Note: It's important to prefix your regex patterns with r to ensure that your patterns are interpreted in the way you want them to. Else, you may encounter problems to do with escape sequences in strings. For example, "\n" in Python is used to indicate a new line, but if you use the r prefix, it will be interpreted as the raw string "\n" - that is, the character "\" followed by the character "n" - and not as a new line.

Instructions
100 XP
Instructions
100 XP
Import the regular expression module re.
Split my_string on each sentence ending. To do this:
Write a pattern called sentence_endings to match sentence endings (., ?, and !).
Use re.split() to split my_string on the pattern and print the result.
Find and print all capitalized words in my_string by writing a pattern called capitalized_words and using re.findall().
Remember the [a-z] pattern shown in the video to match lowercase groups? Modify that pattern appropriately in order to match uppercase groups.
Write a pattern called spaces to match one or more spaces ("\s+") and then use re.split() to split my_string on this pattern, keeping all punctuation intact. Print the result.
Find all digits in my_string by writing a pattern called digits ("\d+") and using re.findall(). Print the result.


# Import the regex module
import re

# Write a pattern to match sentence endings: sentence_endings
sentence_endings = r"[.?!]"

# Split my_string on sentence endings and print the result
print(re.split(sentence_endings, my_string))

# Find all capitalized words in my_string and print the result
capitalized_words = r"[A-Z]\w+"
print(re.findall(capitalized_words, my_string))

# Split my_string on spaces and print the result
spaces = r"\s+"
print(re.split(spaces, my_string))

# Find all digits in my_string and print the result
digits = r"\d+"
print(re.findall(digits, my_string))




________________________________________________________________________________________________________________________


Introduction to
tokenization


What is tokenization?
Turning a string or document into tokens (smaller chunks)
One step in preparing a text for NLP
Many different theories and rules
You can create your own rules using regular expressions
Some examples:
Breaking out words or sentences
Separating punctuation
Separating all hashtags in a tweet


nltk library
nltk: natural language toolkit
In [1]: from nltk.tokenize import word_tokenize
In [2]: word_tokenize("Hi there!")
Out[2]: ['Hi'
,
'there'
,
'!']


Why tokenize?
Easier to map part of speech
Matching common words
Removing unwanted tokens
"I don't like Sam's shoes."
"I"
,
"do"
,
"n't"
,
"like"
,
"Sam"
,
"'s"
,
"shoes"
,
"."


Other nltk tokenizers
sent_tokenize: tokenize a document into sentences
regexp_tokenize: tokenize a string or document based on a regular
expression pattern
TweetTokenizer: special class just for tweet tokenization, allowing you
to separate hashtags, mentions and lots of exclamation points!!!



More regex practice
Difference between re.search() and re.match()
In [1]: import re
In [2]: re.match('abc'
,
'abcde')
Out[2]: <_sre.SRE_Match object; span=(0, 3), match='abc'>
In [3]: re.search('abc'
,
'abcde')
Out[3]: <_sre.SRE_Match object; span=(0, 3), match='abc'>
In [4]: re.match('cd'
,
'abcde')
In [5]: re.search('cd'
,
'abcde')
Out[5]: <_sre.SRE_Match object; span=(2, 4), match='cd'>

_____________________________________________________________________________________________________________________


Word tokenization with NLTK
Here, you'll be using the first scene of Monty Python's Holy Grail, which has been pre-loaded as scene_one. Feel free to check it out in the IPython Shell!

Your job in this exercise is to utilize word_tokenize and sent_tokenize from nltk.tokenize to tokenize both words and sentences from Python strings - in this case, the first scene of Monty Python's Holy Grail.

Instructions
100 XP
Import the sent_tokenize and word_tokenize functions from nltk.tokenize.
Tokenize all the sentences in scene_one using the sent_tokenize() function.
Tokenize the fourth sentence in sentences, which you can access as sentences[3], using the word_tokenize() function.
Find the unique tokens in the entire scene by using word_tokenize() on scene_one and then converting it into a set using set().
Print the unique tokens found. This has been done for you, so hit 'Submit Answer' to see the results!


# Import necessary modules
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize

# Split scene_one into sentences: sentences
sentences = sent_tokenize(scene_one)

# Use word_tokenize to tokenize the fourth sentence: tokenized_sent
tokenized_sent = word_tokenize(sentences[3])

# Make a set of unique tokens in the entire scene: unique_tokens
unique_tokens = set(word_tokenize(scene_one))

# Print the unique tokens result
print(unique_tokens)


<script.py> output:
    {'master', 'seek', 'wants', 'suggesting', 'European', 'climes', 'an', '[', 'Not', 'Please', 'ounce', 'all', 'Court', 'anyway', "'ve", 'matter', 'feathers', 'may', 'here', 'castle', 'weight', 'Mercea', 'forty-three', 'they', ',', 'son', 'using', 'martin', 'times', 'point', 'your', 'its', 'minute', 'two', 'by', 'simple', 'lord', 'ratios', 'It', 'does', 'question', 'through', 'non-migratory', 'but', 'defeator', 'warmer', 'goes', 'Whoa', 'guiding', 'beat', 'you', '...', 'Oh', 'We', 'So', '--', 'not', 'needs', 'must', '?', 'a', 'They', 'why', 'use', 'halves', 'Listen', 'will', 'maintain', 'migrate', 'then', ']', 'temperate', 'line', 'and', 'That', 'swallow', 'breadth', 'one', 'Well', 'agree', 'under', 'pound', 'snows', 'them', "'s", 'do', 'England', 'go', 'wings', 'second', 'our', 'got', 'Yes', 'since', 'African', 'five', 'my', 'grip', 'course', 'yeah', 'Saxons', 'me', 'get', 'strand', 'at', ':', 'air-speed', 'bangin', 'mean', 'he', 'of', 'order', 'KING', 'zone', 'Pull', 'with', 'Supposing', 'creeper', "n't", 'A', 'Will', 'if', 'ask', 'search', 'am', '!', 'other', 'carried', 'coconuts', '#', 'Uther', 'strangers', 'be', 'coconut', 'I', 'dorsal', 'just', 'back', 'held', 'empty', 'SOLDIER', 'bird', 'plover', 'who', 'tell', 'Arthur', 'court', 'Camelot', 'right', 'sovereign', 'wind', 'Who', 'yet', 'south', '2', 'together', 'could', 'is', 'Patsy', 'there', 'or', 'bring', 'Wait', 'No', 'are', '.', 'The', 'it', 'length', 'the', 'Are', 'kingdom', 'SCENE', 'carrying', 'land', 'velocity', 'But', 'You', 'this', 'winter', 'from', "'em", 'to', 'Am', '1', "'m", 'house', 'swallows', 'grips', 'husk', 'servant', "'d", 'sun', 'covered', 'Halt', 'join', 'Pendragon', 'Britons', 'tropical', 'fly', 'King', 'have', 'Where', 'maybe', "'", 'ARTHUR', 'Found', 'every', 'found', 'in', 'where', 'knights', 'ridden', 'interested', 'Ridden', 'carry', 'In', 'these', 'on', 'horse', 'trusty', 'that', 'What', 'clop', 'speak', "'re"}

________________________________________________________________________________________________________________________

More regex with re.search()
In this exercise, you'll utilize re.search() and re.match() to find specific tokens. Both search and match expect regex patterns, similar to those you defined in an earlier exercise. You'll apply these regex library methods to the same Monty Python text from the nltk corpora.

You have both scene_one and sentences available from the last exercise; now you can use them with re.search() and re.match() to extract and match more text.

Instructions
100 XP
Use re.search() to search for the first occurance of the word "coconuts" in scene_one. Store the result in match.
Print the start and end indexes of match using its .start() and .end() methods, respectively.
Write a regular expression called pattern1 to find anything in square brackets.
Use re.search() with the previous pattern to find the first text in square brackets in the scene. Print the result.
Use re.match() to match the script notation in the fourth line (ARTHUR:) and print the result. The tokenized sentences of scene_one are available in your namespace as sentences.


# Search for the first occurrence of "coconuts" in scene_one: match
match = re.search("coconuts", scene_one)

# Print the start and end indexes of match
print(match.start(), match.end())

# Write a regular expression to search for anything in square brackets: pattern1
pattern1 = r"\[.*\]"

# Use re.search to find the first text in square brackets
print(re.search(pattern1, scene_one))

# Find the script notation at the beginning of the fourth sentence and print it
pattern2 = r"[\w\s]+:"
print(re.match(pattern2, sentences[3]))



________________________________________________________________________________________________________________________

Advanced tokenization
with regex

Regex groups using or "|"
OR is represented using |
You can define a group using ()
You can define explicit character ranges using []

In [1]: import re
In [2]: match_digits_and_words = ('(\d+|\w+)')
In [3]: re.findall(match_digits_and_words,
'He has 11 cats.')
Out[3]: ['He'
,
'has'
,
'11'
,
'cats']



Regex ranges and groups
pattern matches example
[A-Za-z]+ upper and lowercase English alphabet 'ABCDEFghijk'
[0-9] numbers from 0 to 9 9
[A-Za-z\-
\.]+
upper and lowercase English alphabet, -
and .
'MyWebsite.com'
(a-z) a, - and z 'a-z'
(\s+l,) spaces or a comma '
,
'



Character range with re.match()
In [1]: import re
In [2]: my_str = 'match lowercase spaces nums like 12, but no commas'
In [3]: re.match('[a-z0-9 ]+'
, my_str)
Out[3]: <_sre.SRE_Match object;
span=(0, 42), match='match lowercase spaces nums like 12'>





________________________________________________________________________________________________________________________

Charting word length
with nltk


Getting started with matplotlib
Charting library used by many open source Python projects
Straightforward functionality with lots of options
Histograms
Bar charts
Line charts
Scatter plots
... and also advanced functionality like 3D graphs and animations!


Plotting a histogram with matplotlib
In [1]: from matplotlib import pyplot as plt
In [2]: plt.hist([1, 5, 5, 7, 7, 7, 9])
Out[2]: (array([ 1., 0., 0., 0., 0., 2., 0., 3., 0., 1.]),
array([ 1. , 1.8, 2.6, 3.4, 4.2, 5. , 5.8, 6.6,
7.4, 8.2, 9. ]),
<a list of 10 Patch objects>)
In [3]: plt.show()


Generated Histogram


Combining NLP data extraction with plotting
In [1]: from matplotlib import pyplot as plt
In [2]: from nltk.tokenize import word_tokenize
In [3]: words = word_tokenize("This is a pretty cool tool!")
In [4]: word_lengths = [len(w) for w in words]
In [5]: plt.hist(word_lengths)
Out[5]: (array([ 2., 0., 1., 0., 0., 0., 3., 0., 0., 1.]),
array([ 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5,
6. ]),
<a list of 10 Patch objects>)
In [6]: plt.show()









________________________________________________________________________________________________________________________




===============================================================================================================================

2
Simple topic identification
0%
This chapter will introduce you to topic identification, which you can apply to any text you encounter in the wild. Using basic NLP models, you will identify topics from texts based on term frequencies. You'll experiment and compare two simple methods - bag-of-words and Tf-idf using NLTK and a new library - Gensim.





===============================================================================================================================
3
Named-entity recognition
0%
This chapter will introduce a slightly more advanced topic - Named-entity recognition. You'll learn how to identify the who, what and where of your texts using pre-trained models on English and non-English text. You'll also learn how to use some new libraries - polyglot and spaCy - to add to your NLP toolbox.






===============================================================================================================================


4
Building a "fake news" classifier
0%
Here, you'll apply the basics of what you've learned along with some supervised machine learning to build a "fake news" detector. You'll begin by learning the basics of supervised machine learning, and then move forward by choosing a few important features and testing ideas to identify and classify "fake news" articles.
