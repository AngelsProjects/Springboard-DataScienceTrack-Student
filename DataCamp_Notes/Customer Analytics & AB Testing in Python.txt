Course Description
The most successful companies today are the ones that know their customers so well that they can anticipate their needs. Customer analytics and in particular A/B Testing are crucial parts of leveraging quantitative know-how to help make business decisions that generate value. This course covers the ins and outs of how to use Python to analyze customer behavior and business trends as well as how to create, run, and analyze A/B tests to make proactive, data-driven business decisions.

_____________________________________________________________________________________________________________________________________
1
Key Performance Indicators: Measuring Business Success
FREE
0%
This chapter provides a brief introduction to the content that will be covered throughout the course before transitioning into a discussion of Key Performance Indicators or KPIs. You'll learn how to identify and define meaningful KPIs through a combination of critical thinking and leveraging Python tools. These techniques are all presented in a highly practical and generalizable way. Ultimately these topics serve as the core foundation for the A/B testing discussion that follows.


_____________________________________________________________________________________________________________________________________

Practicing aggregations
It's time to begin exploring the in-app purchase data in more detail. Here, you will practice aggregating the dataset in various ways using the .agg() method and then examine the results to get an understanding of the overall data, as well as a feel for how to aggregate data using pandas.

Loaded for you is a DataFrame named purchase_data which is the dataset of in-app purchase data merged with the user demographics data from earlier.

Before getting started, it's good practice to explore this purchase_data DataFrame in the IPython Shell. In particular, notice the price column: you'll examine it further in this exercise.

Instructions 1/3
30 XP
1
Find the 'mean' purchase price paid across our dataset. Then examine the output before moving on.

# Calculate the mean purchase price 
purchase_price_mean = purchase_data.price.agg('mean')

# Examine the output 
print(purchase_price_mean)


Now, use the .agg() method to find the 'mean' and 'median' prices together.

# Calculate the mean and median purchase price 
purchase_price_summary = purchase_data.price.agg(['mean', 'median'])

# Examine the output 
print(purchase_price_summary)


Now, find the 'mean' and 'median' for both the 'price' paid and the 'age' of purchaser.

# Calculate the mean and median of price and age
purchase_summary = purchase_data.agg({'price': ['mean', 'median'], 'age': ['mean', 'median']})

# Examine the output 
print(purchase_summary)

_____________________________________________________________________________________________________________________________________

rouping & aggregating
You'll be using .groupby() and .agg() a lot in this course, so it's important to become comfortable with them. In this exercise, your job is to calculate a set of summary statistics about the purchase data broken out by 'device' (Android or iOS) and 'gender' (Male or Female).

Following this, you'll compare the values across these subsets, which will give you a baseline for these values as potential KPIs to optimize going forward.

The purchase_data DataFrame from the previous exercise has been pre-loaded for you. As a reminder, it contains purchases merged with user demographics.

Instructions
100 XP
Instructions
100 XP
Group the purchase_data DataFrame by 'device' and 'gender' in that order.
Aggregate grouped_purchase_data, finding the 'mean', 'median', and the standard deviation ('std') of the purchase price, in that order, across these groups.
Examine the results. Does the mean differ drastically from the median? How much variability is in each group?

# Group the data 
grouped_purchase_data = purchase_data.groupby(by = ['device', 'gender'])

# Aggregate the data
purchase_summary = grouped_purchase_data.agg({'price': ['mean', 'median', 'std']})

# Examine the results
print(purchase_summary)


<script.py> output:
                        price                   
                         mean median         std
    device gender                               
    and    F       400.747504    299  179.984378
           M       416.237308    499  195.001520
    iOS    F       404.435330    299  181.524952
           M       405.272401    299  196.843197


_____________________________________________________________________________________________________________________________________

Average purchase price by cohort
Building on the previous exercise, let's look at the same KPI, average purchase price, and a similar one, median purchase price, within the first 28 days. Additionally, let's look at these metrics not limited to 28 days to compare.

We can calculate these metrics across a set of cohorts and see what differences emerge. This is a useful task as it can help us understand how behaviors vary across cohorts.

Note that in our data the price variable is given in cents.

Instructions 1/3
0 XP
1
2
3
Instructions 1/3
0 XP
1
2
3
Use np.where to create an array month1 containing:

the price of the purchase purchase, if

the user registration .reg_date occurred at most 28 days ago (i.e. before max_reg_date), and

the date of purchase .date occurred within 28 days of registration date .reg_date;

NaN, otherwise.

Hint
the two date conditions are:

purchase_data.reg_date is less than max_reg_date

purchase_data.date is less than purchase_data.reg_date plus 28 days.

You can type np.where? in console to find out more about the function and how to use it.


# Set the max registration date to be one month before today
max_reg_date = current_date - timedelta(days=28)

# Find the month 1 values
month1 = np.where((purchase_data.reg_date < max_reg_date) &
                 (purchase_data.date < purchase_data.reg_date + timedelta(days=28)),
                  purchase_data.price, 
                  np.NaN)
                 
# Update the value in the DataFrame
purchase_data['month1'] = month1


Now, group purchase_data by gender and then device using the .groupby() method.


# Set the max registration date to be one month before today
max_reg_date = current_date - timedelta(days=28)

# Find the month 1 values
month1 = np.where((purchase_data.reg_date < max_reg_date) &
                 (purchase_data.date < purchase_data.reg_date + timedelta(days=28)),
                  purchase_data.price, 
                  np.NaN)
                 
# Update the value in the DataFrame
purchase_data['month1'] = month1

# Group the data by gender and device 
purchase_data_upd = purchase_data.groupby(by=['gender', 'device'], as_index=False)


Aggregate the "mean" and "median" of both 'month1' and'price' using the .agg() method in the listed order of aggregations and fields.

# Set the max registration date to be one month before today
max_reg_date = current_date - timedelta(days=28)

# Find the month 1 values
month1 = np.where((purchase_data.reg_date < max_reg_date) &
                 (purchase_data.date < purchase_data.reg_date + timedelta(days=28)),
                  purchase_data.price, 
                  np.NaN)
                 
# Update the value in the DataFrame
purchase_data['month1'] = month1

# Group the data by gender and device 
purchase_data_upd = purchase_data.groupby(by=['gender', 'device'], as_index=False) 

# Aggregate the month1 and price data 
purchase_summary = purchase_data_upd.agg(
                        {'month1': ['mean', 'median'],
                        'price': ['mean', 'median']})

# Examine the results 
print(purchase_summary)

<script.py> output:
      gender device      month1              price       
                           mean median        mean median
    0      F    and  388.204545  299.0  400.747504    299
    1      F    iOS  432.587786  499.0  404.435330    299
    2      M    and  413.705882  399.0  416.237308    499
    3      M    iOS  433.313725  499.0  405.272401    299
		
		
Great! This value seems relatively stable over the past 28 days. Congratulations on completing Chapter 1! In the next chapter, you'll explore and visualize customer behavior in more detail.


_____________________________________________________________________________________________________________________________________

2
Exploring and Visualizing Customer Behavior
0%
This chapter teaches you how to visualize, manipulate, and explore KPIs as they change over time. Through a variety of examples, you'll learn how to work with datetime objects to calculate metrics per unit time. Then we move to the techniques for how to graph different segments of data, and apply various smoothing functions to reveal hidden trends. Finally we walk through a complete example of how to pinpoint issues through exploratory data analysis of customer data. Throughout this chapter various functions are introduced and explained in a highly generalizable way.


_____________________________________________________________________________________________________________________________________
Using the Timedelta Class
current_date = pd.to_datetime('2018-03-17')
max_lapse_date = current_date - timedelta(days=14)
conv_sub_data = sub_data_demo[sub_data_demo.lapse_date < max_lapse_date]



Date Differences
sub_time = (conv_sub_data.subscription_date
- conv_sub_data.lapse_date)
conv_sub_data['sub_time'] = sub_time

Date Components
conv_sub_data['sub_time'] = conv_sub_data.sub_time.dt.days



Conversion Rate Calculation
conv_base = conv_sub_data[(conv_sub_data.sub_time.notnull()) | (conv_sub_data.sub_time > 7)]
total_users = len(conv_base)
2086


total_subs = np.where(conv_sub_data.sub_time.notnull() & (conv_base.sub_time <= 14), 1, 0)
total_subs = sum(total_subs)
20



conversion_rate = total_subs / total_users
0.0095877277085330784

Parsing Dates - On Import
pandas.read_csv(...,
parse_dates=False,
infer_datetime_format=False,
keep_date_col=False,
date_parser=None,
dayfirst=False,...)


customer_demographics = pd.read_csv('customer_demographics.csv',
parse_dates=True,
infer_datetime_format=True)


uid reg_date device gender country age
0 54030035.0 2017-06-29 and M USA 19
1 72574201.0 2018-03-05 iOS F TUR 22
2 64187558.0 2016-02-07 iOS M USA 16


Parsing Dates - Manually
pandas.to_datetime(arg, errors=
'raise'
, ..., format=None, ...)
strftime
1993-01-27 -- "%Y-%m-%d"
05/13/2017 05:45:37 -- "%m/%d/%Y %H:%M:%S"
September 01, 2017 -- "%B %d, %Y"

_____________________________________________________________________________________________________________________________________

Creating time series
graphs With
matplotlib
CUS TOMER AN A LYTICS & A /B TES TIN G IN PYTH ON


Conversion Rate by Day
current_date = pd.to_datetime('2018-03-17')
max_lapse_date = current_date - timedelta(days=7)
conv_sub_data = sub_data_demo[sub_data_demo.lapse_date
< max_lapse_date]
sub_time = (conv_sub_data.subscription_date -
conv_sub_data.lapse_date).dt.days
conv_sub_data['sub_time'] = sub_time

Conversion Rate by Day
conversion_data = conv_sub_data.groupby(by=['lapse_date'],
as_index=False)
conversion_data = conversion_data.agg({'sub_time': [gc7]})
conversion_data.columns = conversion_data.columns.droplevel(
level=1)
conversion_data.head()
lapse_date sub_time
0 2017-09-01 0.224775
1 2017-09-02 0.223749
2 2017-09-03 0.222948
3 2017-09-04 0.222222
4 2017-09-05 0.229401

Plotting Daily Conversion Rate
conversion_data.lapse_date =
pd.to_datetime(conversion_data.lapse_date)
conversion_data.plot(x=
'lapse_date'
, y=
'sub_time')


Plotting Daily Conversion Rate
plt.show()


Trends in Different Cohorts
conversion_data.head()
lapse_date country sub_time
0 2017-09-01 BRA 0.184000
1 2017-09-01 CAN 0.285714
2 2017-09-01 DEU 0.276119
3 2017-09-01 FRA 0.240506
4 2017-09-01 TUR 0.161905



`
.pivot_table()`
Pivot Table Method
pandas.pivot_table(
data, values=None, index=None, columns=None,
aggfunc=
'mean'
, fill_value=None, margins=False,
dropna=True, margins_name=
'All')



`
.pivot_table()`
reformatted_cntry_data =pd.pivot_table(conversion_data, ...)
reformatted_cntry_data =pd.pivot_table(conversion_data,
values=['sub_time'],...)
reformatted_cntry_data =pd.pivot_table(conversion_data,
values=['sub_time'], columns=['country'],
...)
reformatted_cntry_data =pd.pivot_table(conversion_data,
values=['sub_time'],columns=['country'],
index=['reg_date'],fill_value=0 )


`
.pivot_table()`
reformatted_cntry_data.columns
reformatted_cntry_data.columns.droplevel(level=[0])
reformatted_cntry_data.reset_index(inplace=True)
reformatted_cntry_data.head()


lapse_date BRA CAN DEU
2017-09-01 0.184000 0.285714 0.276119 ...
2017-09-02 0.171296 0.244444 0.276190 ...
2017-09-03 0.177305 0.295082 0.266055 ...


Plotting Trends in Different Cohorts
reformatted_cntry_data.plot(
x=
'reg_date'
,
y=['BRA'
,
'FRA'
,
'DEU'
,
'TUR'
,
'USA'
,
'CAN']
)
plt.show()

_____________________________________________________________________________________________________________________________________


Understanding and
visualizing trends in
customer data



Subscribers Per Day
usa_subscriptions = pd.read_csv('usa_subscribers.csv',
parse_dates=True,
infer_datetime_format=True)
usa_subscriptions['sub_day'] = (usa_subscriptions.sub_date -
usa_subscriptions.lapse_date).dt.days
usa_subscriptions = usa_subscriptions[usa_subscriptions.sub_day <= 7]
usa_subscriptions = usa_subscriptions.groupby(by=['sub_date'], as_index = False)
usa_subscriptions = usa_subscriptions.agg({'subs': ['sum']})
usa_subscriptions.columns = usa_subscriptions.columns.droplevel(level=[1])
usa_subscriptions.head()




sub_date subs
0 2016-09-02 37
1 2016-09-03 50
2 2016-09-04 59


Subscribers Per Day
usa_subscriptions.plot(x=
'sub_date'
, y=
'subs')
plt.show()


Calculating Trailing Averages
rolling_subs = usa_subscriptions.subs.rolling(...)
rolling_subs = usa_subscriptions.subs.rolling(window=7, ...)
In [10]: rolling_subs = usa_subscriptions.subs.rolling(
window=7, center=False)


Calculating Trailing Averages
rolling_subs = rolling_subs.mean()
usa_subscriptions['rolling_subs'] = rolling_subs
usa_subscriptions.tail()
sub_date subs rolling_subs
2018-03-14 89 94.714286
2018-03-15 96 95.428571
2018-03-16 102 96.142857
2018-03-17 102 96.142857
2018-03-18 115 98.714286





Noisy Data
high_sku_purchases = pd.read_csv('high_sku_purchases.csv'
,
parse_dates=True,
infer_datetime_format=True)
high_sku_purchases.plot(x=
'date'
, y=
'purchases')
plt.show()




Calculating an Exponential Moving Average
exp_mean = high_sku_purchases.purchases.ewm(span=30)
exp_mean = exp_mean.mean()
high_sku_purchases['exp_mean'] = exp_mean


Calculating an Exponential Moving Average
high_sku_purchases.plot(x=
'date'
, y=
'exp_mean')

_____________________________________________________________________________________________________________________________________

Exploratory data
analysis with time
series data


Drop in New User Retention
current_date = pd.to_datetime('2018-03-17')
max_lapse_date = current_date - timedelta(days=7)
conv_sub_data = sub_data_demo[sub_data_demo.lapse_date
<= max_lapse_date]
sub_time = (conv_sub_data.subscription_date -
conv_sub_data.lapse_date).dt.days
conv_sub_data['sub_time'] = sub_time
conversion_data = conv_sub_data.groupby(by=['lapse_date'],
as_index=False)
conversion_data = conversion_data.agg({'sub_time': [gc7]})
conversion_data.columns = conversion_data.columns.droplevel(level=1)


conversion_data.plot()
plt.show()


Limiting our View
current_date = pd.to_datetime('2018-03-17')
start_date = current_date - timedelta(days=(6*28))
conv_filter = ((conversion_data.lapse_date >= start_date) &
(conversion_data.lapse_date <= current_date))
conversion_data_filt = conversion_data[conv_filter]
conversion_data_filt.plot(x=
'lapse_date'
, y=
'sub_time')

plt.show()



Splitting by Country & Device
conv_filter = ((conv_sub_data.lapse_date >= start_date) &
(conv_sub_data.lapse_date <= current_date))
conv_data = conv_sub_data[conv_filter]
conv_data_cntry = conv_data.groupby(by=['lapse_date', 'country'],
as_index=False)
conv_data_cntry = conv_data_cntry.agg({'sub_time': [gc7]})
conv_data_cntry.columns = conv_data_cntry.columns.droplevel(level=1)
conv_data_cntry = pd.pivot_table(conv_data_cntry,
values=['sub_time'],
columns=['country'],
index=['lapse_date'],fill_value=0 )
conv_data_cntry.columns = conv_data_cntry.columns.droplevel(
level=0)
conv_data_cntry.reset_index(inplace=True)
conv_data_cntry.plot(x=['lapse_date'],
y=['BRA', 'CAN', 'DEU', 'FRA', 'TUR', 'USA'])
plt.show()




Splitting by Country & Device
conv_filter = ((conv_sub_data.lapse_date >= start_date) &
(conv_sub_data.lapse_date <= current_date))
conv_data = conv_sub_data[conv_filter]
conv_data_dev = conv_data.groupby(by=['lapse_date',
'device'], as_index=False)
conv_data_dev = conv_data_dev.agg({'sub_time': [gc7]})
conv_data_dev.columns = conv_data_dev.columns.droplevel(level=1)
conv_data_dev = pd.pivot_table(conv_data_dev,
values=['sub_time'],
columns=['device'],
index=['lapse_date'],fill_value=0 )
conv_data_dev.columns = conv_data_dev.columns.droplevel(level=[0])
conv_data_dev.reset_index(inplace=True)
conv_data_dev.plot(x=['lapse_date'],
y=['iOS', 'and'])
plt.show()


Annotation Datasets
events = pd.read_csv('events.csv')
events.head()

Date Event
2018-01-01 NYD
2017-01-01 NYD
2016-01-01 NYD
2015-01-01 NYD
2014-01-01 NYD


releases = pd.read_csv('releases.csv')
releases.head()

Date Event
2018-03-14 iOS Release
2018-03-03 Android Release
2018-01-13 iOS Release
2018-01-15 Android Release
2017-11-03 Android Release





Plotting Annotations
conv_data_dev.plot(x=['lapse_date'], y=['iOS'
,
'and'])

events.Date = pd.to_datetime(events.Date)

for row in events.iterrows():
tmp = row[1]
plt.axvline(x=tmp.Date, color=
'k'
, linestyle=
'--')



Plotting Annotations
releases.Date = pd.to_datetime(releases.Date)
for row in releases.iterrows():
tmp = row[1]
if tmp.Event ==
'iOS Release':
plt.axvline(x=tmp.Date, color=
'b'
, linestyle=
'--')
else:
plt.axvline(x=tmp.Date, color=
'r'
, linestyle=
'--')
plt.show()





_____________________________________________________________________________________________________________________________________

3
The Design and Application of A/B Testing
0%
In this chapter you will dive fully into A/B testing. You will learn the mathematics and knowledge needed to design and successfully plan an A/B test from determining an experimental unit to finding how large a sample size is needed. Accompanying this will be an introduction to the functions and code needed to calculate the various quantities associated with a statistical test of this type.





_____________________________________________________________________________________________________________________________________

4
Analyzing A/B Testing Results
0%
After running an A/B test, you must analyze the data and then effectively communicate the results. This chapter begins by interleaving the theory of statistical significance and confidence intervals with the tools you need to calculate them yourself from the data. Next we discuss how to effectively visualize and communicate these results. This chapter is the culmination of all the knowledge built over the entire course.

