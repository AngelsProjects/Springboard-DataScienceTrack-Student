1
Exploring your data


So you've just got a brand new dataset and are itching to start exploring it. But where do you begin, and how can you be sure your dataset is clean? This chapter will introduce you to the world of data cleaning in Python! You'll learn how to explore your data with an eye for diagnosing issues such as outliers, missing values, and duplicate rows.

______________________________________________________________________________________________________________


Loading and viewing your data
In this chapter, you're going to look at a subset of the Department of Buildings Job Application Filings dataset from the NYC Open Data portal. This dataset consists of job applications filed on January 22, 2017.

Your first task is to load this dataset into a DataFrame and then inspect it using the .head() and .tail() methods. However, you'll find out very quickly that the printed results don't allow you to see everything you need, since there are too many columns. Therefore, you need to look at the data in another way.

The .shape and .columns attributes let you see the shape of the DataFrame and obtain a list of its columns. From here, you can see which columns are relevant to the questions you'd like to ask of the data. To this end, a new DataFrame, df_subset, consisting only of these relevant columns, has been pre-loaded. This is the DataFrame you'll work with in the rest of the chapter.

Get acquainted with the dataset now by exploring it with pandas! This initial exploratory analysis is a crucial first step of data cleaning.

Instructions
50 XP
Instructions
50 XP
Import pandas as pd.
Read 'dob_job_application_filings_subset.csv' into a DataFrame called df.
Print the head and tail of df.
Print the shape of df and its columns. Note: .shape and .columns are attributes, not methods, so you don't need to follow these with parentheses ().
Hit 'Submit Answer' to view the results! Notice the suspicious number of 0 values. Perhaps these represent missing data.



# Import pandas
import pandas as pd

# Read the file into a DataFrame: df
df = pd.read_csv('dob_job_application_filings_subset.csv')

# Print the head of df
print(df.head())

# Print the tail of df
print(df.tail())

# Print the shape of df
print(df.shape)

# Print the columns of df
print(df.columns)

# Print the head and tail of df_subset
print(df_subset.head())
print(df_subset.tail())


<script.py> output:
           Job #  Doc #        Borough       House #  \
    0  121577873      2      MANHATTAN  386            
    1  520129502      1  STATEN ISLAND  107            
    2  121601560      1      MANHATTAN  63             
    3  121601203      1      MANHATTAN  48             
    4  121601338      1      MANHATTAN  45             
    
                            Street Name  Block  Lot    Bin # Job Type Job Status  \
    0  PARK AVENUE SOUTH                   857   38  1016890       A2          D   
    1  KNOX PLACE                          342    1  5161350       A3          A   
    2  WEST 131 STREET                    1729    9  1053831       A2          Q   
    3  WEST 25TH STREET                    826   69  1015610       A2          D   
    4  WEST 29 STREET                      831    7  1015754       A3          D   
    
                ...                         Owner's Last Name  \
    0           ...            MIGLIORE                         
    1           ...            BLUMENBERG                       
    2           ...            MARKOWITZ                        
    3           ...            CASALE                           
    4           ...            LEE                              
    
                  Owner's Business Name Owner's House Number  \
    0  MACKLOWE MANAGEMENT                      126            
    1  NA                                       107            
    2  635 RIVERSIDE DRIVE NY LLC               619            
    3  48 W 25 ST LLC C/O BERNSTEIN             150            
    4  HYUNG-HYANG REALTY CORP                  614            
    
               Owner'sHouse Street Name            City  State    Zip  \
    0  EAST 56TH STREET                  NEW YORK           NY  10222   
    1  KNOX PLACE                        STATEN ISLAND      NY  10314   
    2  WEST 54TH STREET                  NEW YORK           NY  10016   
    3  WEST 30TH STREET                  NEW YORK           NY  10001   
    4  8 AVENUE                          NEW YORK           NY  10001   
    
      Owner'sPhone #                                    Job Description  \
    0     2125545837  GENERAL MECHANICAL & PLUMBING MODIFICATIONS AS...   
    1     3477398892  BUILDERS PAVEMENT PLAN 143 LF.                ...   
    2     2127652555  GENERAL CONSTRUCTION TO INCLUDE NEW PARTITIONS...   
    3     2125941414  STRUCTURAL CHANGES ON THE 5TH FLOOR (MOONDOG E...   
    4     2019881222  FILING HEREWITH FACADE REPAIR PLANS. WORK SCOP...   
    
                   DOBRunDate  
    0  04/26/2013 12:00:00 AM  
    1  04/26/2013 12:00:00 AM  
    2  04/26/2013 12:00:00 AM  
    3  04/26/2013 12:00:00 AM  
    4  04/26/2013 12:00:00 AM  
    
    [5 rows x 82 columns]
               Job #  Doc #        Borough       House #  \
    12841  520143988      1  STATEN ISLAND  8              
    12842  121613833      1      MANHATTAN  724            
    12843  121681260      1      MANHATTAN  350            
    12844  320771704      1       BROOKLYN  499            
    12845  520143951      1  STATEN ISLAND  1755           
    
                                Street Name  Block  Lot    Bin # Job Type  \
    12841  NOEL STREET                        5382   20  5069722       A2   
    12842  10 AVENUE                          1059    4  1082503       A2   
    12843  MANHATTAN AVE.                     1848   31  1055849       A2   
    12844  UNION STREET                        431   43  3007185       A2   
    12845  RICHMOND ROAD                       887   28  5022931       A2   
    
          Job Status           ...                         Owner's Last Name  \
    12841          D           ...            MALITO                           
    12842          D           ...            CROMAN                           
    12843          A           ...            ARYEH                            
    12844          D           ...            WIGGINS                          
    12845          D           ...            CAMBRIA                          
    
                      Owner's Business Name Owner's House Number  \
    12841  GENO MALITO                              8              
    12842  722-724 10TH AVENUE HOLDING LLC          632            
    12843  DG UWS LLC                               619            
    12844  N/A                                      77             
    12845  RONALD CAMBRIA                           1755           
    
                   Owner'sHouse Street Name            City  State    Zip  \
    12841  NOEL STREET                       STATEN ISLAND      NY  10312   
    12842  BROADWAY                          NEW YORK           NY  10012   
    12843  WEST 54TH STREET                  NEW YORK           NY  10019   
    12844  PROSPECT PLACE                    BROOKLYN           NY  11217   
    12845  RICHMOND ROAD                     STATEN ISLAND      NY  10304   
    
          Owner'sPhone #                                    Job Description  \
    12841     9174685659  HORIZONTAL ENLARGEMENT OF ATTACHED ONE CAR GAR...   
    12842     2122289300  RENOVATION OF EXISTING APARTMENT #3B ON THIRD ...   
    12843     2127652555  REPLACE BURNER IN EXSTG BOILER WITH NEW GAS BU...   
    12844     9178487799  INSTALL NEW SPRINKLER SYSTEM THROUGHOUT THE BU...   
    12845     7184482740  INTERIOR PARTITIONS AND MINOR PLUMBING WORK TO...   
    
                       DOBRunDate  
    12841  06/13/2013 12:00:00 AM  
    12842  06/13/2013 12:00:00 AM  
    12843  06/13/2013 12:00:00 AM  
    12844  06/13/2013 12:00:00 AM  
    12845  06/13/2013 12:00:00 AM  
    
    [5 rows x 82 columns]
    (12846, 82)
    Index(['Job #', 'Doc #', 'Borough', 'House #', 'Street Name', 'Block', 'Lot',
           'Bin #', 'Job Type', 'Job Status', 'Job Status Descrp',
           'Latest Action Date', 'Building Type', 'Community - Board', 'Cluster',
           'Landmarked', 'Adult Estab', 'Loft Board', 'City Owned', 'Little e',
           'PC Filed', 'eFiling Filed', 'Plumbing', 'Mechanical', 'Boiler',
           'Fuel Burning', 'Fuel Storage', 'Standpipe', 'Sprinkler', 'Fire Alarm',
           'Equipment', 'Fire Suppression', 'Curb Cut', 'Other',
           'Other Description', 'Applicant's First Name', 'Applicant's Last Name',
           'Applicant Professional Title', 'Applicant License #',
           'Professional Cert', 'Pre- Filing Date', 'Paid', 'Fully Paid',
           'Assigned', 'Approved', 'Fully Permitted', 'Initial Cost',
           'Total Est. Fee', 'Fee Status', 'Existing Zoning Sqft',
           'Proposed Zoning Sqft', 'Horizontal Enlrgmt', 'Vertical Enlrgmt',
           'Enlargement SQ Footage', 'Street Frontage', 'ExistingNo. of Stories',
           'Proposed No. of Stories', 'Existing Height', 'Proposed Height',
           'Existing Dwelling Units', 'Proposed Dwelling Units',
           'Existing Occupancy', 'Proposed Occupancy', 'Site Fill', 'Zoning Dist1',
           'Zoning Dist2', 'Zoning Dist3', 'Special District 1',
           'Special District 2', 'Owner Type', 'Non-Profit', 'Owner's First Name',
           'Owner's Last Name', 'Owner's Business Name', 'Owner's House Number',
           'Owner'sHouse Street Name', 'City ', 'State', 'Zip', 'Owner'sPhone #',
           'Job Description', 'DOBRunDate'],
          dtype='object')
           Job #  Doc #        Borough Initial Cost Total Est. Fee  \
    0  121577873      2      MANHATTAN    $75000.00        $986.00   
    1  520129502      1  STATEN ISLAND        $0.00       $1144.00   
    2  121601560      1      MANHATTAN    $30000.00        $522.50   
    3  121601203      1      MANHATTAN     $1500.00        $225.00   
    4  121601338      1      MANHATTAN    $19500.00        $389.50   
    
       Existing Zoning Sqft  Proposed Zoning Sqft  Enlargement SQ Footage  \
    0                     0                     0                       0   
    1                     0                     0                       0   
    2                     0                     0                       0   
    3                     0                     0                       0   
    4                     0                     0                       0   
    
       Street Frontage  ExistingNo. of Stories  Proposed No. of Stories  \
    0                0                       0                        0   
    1              143                       0                        0   
    2                0                       5                        5   
    3                0                      12                       12   
    4                0                       6                        6   
    
       Existing Height  Proposed Height  
    0                0                0  
    1                0                0  
    2               54               54  
    3              120              120  
    4               64               64  
               Job #  Doc #        Borough Initial Cost Total Est. Fee  \
    12841  520143988      1  STATEN ISLAND    $30700.00        $448.62   
    12842  121613833      1      MANHATTAN    $62000.00        $852.10   
    12843  121681260      1      MANHATTAN   $166000.00       $1923.30   
    12844  320771704      1       BROOKLYN    $65000.00        $883.00   
    12845  520143951      1  STATEN ISLAND     $9500.00        $316.50   
    
           Existing Zoning Sqft  Proposed Zoning Sqft  Enlargement SQ Footage  \
    12841                  1490                  1782                     206   
    12842                     0                     0                       0   
    12843                     0                     0                       0   
    12844                     0                     0                       0   
    12845                     0                     0                       0   
    
           Street Frontage  ExistingNo. of Stories  Proposed No. of Stories  \
    12841                0                       1                        1   
    12842                0                       5                        5   
    12843                0                       6                        6   
    12844                0                       1                        1   
    12845                0                       1                        1   
    
           Existing Height  Proposed Height  
    12841               10               10  
    12842               55               55  
    12843               64               64  
    12844               18               18  
    12845               18               18
    
    
    
    _______________________________________________________________________________________________________________________
    

Further diagnosis
In the previous exercise, you identified some potentially unclean or missing data. Now, you'll continue to diagnose your data with the very useful .info() method.

The .info() method provides important information about a DataFrame, such as the number of rows, number of columns, number of non-missing values in each column, and the data type stored in each column. This is the kind of information that will allow you to confirm whether the 'Initial Cost' and 'Total Est. Fee' columns are numeric or strings. From the results, you'll also be able to see whether or not all columns have complete data in them.

The full DataFrame df and the subset DataFrame df_subset have been pre-loaded. Your task is to use the .info() method on these and analyze the results.

Instructions
50 XP
Print the info of df.
Print the info of the subset dataframe, df_subset.

# Print the info of df
print(df.info())

# Print the info of df_subset
print(df_subset.info())


<script.py> output:
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 12846 entries, 0 to 12845
    Data columns (total 82 columns):
    Job #                           12846 non-null int64
    Doc #                           12846 non-null int64
    Borough                         12846 non-null object
    House #                         12846 non-null object
    Street Name                     12846 non-null object
    Block                           12846 non-null int64
    Lot                             12846 non-null int64
    Bin #                           12846 non-null int64
    Job Type                        12846 non-null object
    Job Status                      12846 non-null object
    Job Status Descrp               12846 non-null object
    Latest Action Date              12846 non-null object
    Building Type                   12846 non-null object
    Community - Board               12846 non-null object
    Cluster                         0 non-null float64
    Landmarked                      2067 non-null object
    Adult Estab                     1 non-null object
    Loft Board                      65 non-null object
    City Owned                      1419 non-null object
    Little e                        365 non-null object
    PC Filed                        0 non-null float64
    eFiling Filed                   12846 non-null object
    Plumbing                        12846 non-null object
    Mechanical                      12846 non-null object
    Boiler                          12846 non-null object
    Fuel Burning                    12846 non-null object
    Fuel Storage                    12846 non-null object
    Standpipe                       12846 non-null object
    Sprinkler                       12846 non-null object
    Fire Alarm                      12846 non-null object
    Equipment                       12846 non-null object
    Fire Suppression                12846 non-null object
    Curb Cut                        12846 non-null object
    Other                           12846 non-null object
    Other Description               12846 non-null object
    Applicant's First Name          12846 non-null object
    Applicant's Last Name           12846 non-null object
    Applicant Professional Title    12846 non-null object
    Applicant License #             12846 non-null object
    Professional Cert               6908 non-null object
    Pre- Filing Date                12846 non-null object
    Paid                            11961 non-null object
    Fully Paid                      11963 non-null object
    Assigned                        3817 non-null object
    Approved                        4062 non-null object
    Fully Permitted                 1495 non-null object
    Initial Cost                    12846 non-null object
    Total Est. Fee                  12846 non-null object
    Fee Status                      12846 non-null object
    Existing Zoning Sqft            12846 non-null int64
    Proposed Zoning Sqft            12846 non-null int64
    Horizontal Enlrgmt              231 non-null object
    Vertical Enlrgmt                142 non-null object
    Enlargement SQ Footage          12846 non-null int64
    Street Frontage                 12846 non-null int64
    ExistingNo. of Stories          12846 non-null int64
    Proposed No. of Stories         12846 non-null int64
    Existing Height                 12846 non-null int64
    Proposed Height                 12846 non-null int64
    Existing Dwelling Units         12846 non-null object
    Proposed Dwelling Units         12846 non-null object
    Existing Occupancy              12846 non-null object
    Proposed Occupancy              12846 non-null object
    Site Fill                       8641 non-null object
    Zoning Dist1                    11263 non-null object
    Zoning Dist2                    1652 non-null object
    Zoning Dist3                    88 non-null object
    Special District 1              3062 non-null object
    Special District 2              848 non-null object
    Owner Type                      0 non-null float64
    Non-Profit                      971 non-null object
    Owner's First Name              12846 non-null object
    Owner's Last Name               12846 non-null object
    Owner's Business Name           12846 non-null object
    Owner's House Number            12846 non-null object
    Owner'sHouse Street Name        12846 non-null object
    City                            12846 non-null object
    State                           12846 non-null object
    Zip                             12846 non-null int64
    Owner'sPhone #                  12846 non-null int64
    Job Description                 12699 non-null object
    DOBRunDate                      12846 non-null object
    dtypes: float64(3), int64(15), object(64)
    memory usage: 8.0+ MB
    None
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 12846 entries, 0 to 12845
    Data columns (total 13 columns):
    Job #                      12846 non-null int64
    Doc #                      12846 non-null int64
    Borough                    12846 non-null object
    Initial Cost               12846 non-null object
    Total Est. Fee             12846 non-null object
    Existing Zoning Sqft       12846 non-null int64
    Proposed Zoning Sqft       12846 non-null int64
    Enlargement SQ Footage     12846 non-null int64
    Street Frontage            12846 non-null int64
    ExistingNo. of Stories     12846 non-null int64
    Proposed No. of Stories    12846 non-null int64
    Existing Height            12846 non-null int64
    Proposed Height            12846 non-null int64
    dtypes: int64(10), object(3)
    memory usage: 1.3+ MB
    None
    
    
    
    
    
________________________________________________________________________________________________________________________


    
    Frequency counts for categorical data
As you've seen, .describe() can only be used on numeric columns. So how can you diagnose data issues when you have categorical data? One way is by using the .value_counts() method, which returns the frequency counts for each unique value in a column!

This method also has an optional parameter called dropna which is True by default. What this means is if you have missing data in a column, it will not give a frequency count of them. You want to set the dropna column to False so if there are missing values in a column, it will give you the frequency counts.

In this exercise, you're going to look at the 'Borough', 'State', and 'Site Fill' columns to make sure all the values in there are valid. When looking at the output, do a sanity check: Are all values in the 'State' column from NY, for example? Since the dataset consists of applications filed in NY, you would expect this to be the case.

Instructions
100 XP
Instructions
100 XP
Print the value counts for:
The 'Borough' column.
The 'State' column.
The 'Site Fill' column.


# Print the value counts for 'Borough'
print(df['Borough'].value_counts(dropna=False))

# Print the value_counts for 'State'
print(df['State'].value_counts(dropna=False))

# Print the value counts for 'Site Fill'
print(df['Site Fill'].value_counts(dropna=False))

<script.py> output:
    MANHATTAN        6310
    BROOKLYN         2866
    QUEENS           2121
    BRONX             974
    STATEN ISLAND     575
    Name: Borough, dtype: int64
    NY    12391
    NJ      241
    PA       38
    CA       20
    OH       19
    IL       17
    FL       17
    CT       16
    TX       13
    TN       10
    MD        7
    DC        7
    MA        6
    GA        6
    KS        6
    VA        5
    CO        4
    SC        3
    WI        3
    MN        3
    AZ        3
    NC        2
    RI        2
    UT        2
    VT        1
    WA        1
    MI        1
    IN        1
    NM        1
    Name: State, dtype: int64
    NOT APPLICABLE                              7806
    NaN                                         4205
    ON-SITE                                      519
    OFF-SITE                                     186
    USE UNDER 300 CU.YD                          130
    Name: Site Fill, dtype: int64
    
    
    
    
    
    _____________________________________________________________________________________________________________________
    
    Visualizing single variables with histograms
Up until now, you've been looking at descriptive statistics of your data. One of the best ways to confirm what the numbers are telling you is to plot and visualize the data.

You'll start by visualizing single variables using a histogram for numeric values. The column you will work on in this exercise is 'Existing Zoning Sqft'.

The .plot() method allows you to create a plot of each column of a DataFrame. The kind parameter allows you to specify the type of plot to use - kind='hist', for example, plots a histogram.

In the IPython Shell, begin by computing summary statistics for the 'Existing Zoning Sqft' column using the .describe() method. You'll notice that there are extremely large differences between the min and max values, and the plot will need to be adjusted accordingly. In such cases, it's good to look at the plot on a log scale. The keyword arguments logx=True or logy=True can be passed in to .plot() depending on which axis you want to rescale.

Finally, note that Python will render a plot such that the axis will hold all the information. That is, if you end up with large amounts of whitespace in your plot, it indicates counts or values too small to render.

Instructions
100 XP
Instructions
100 XP
Import matplotlib.pyplot as plt.
Create a histogram of the 'Existing Zoning Sqft' column. Rotate the axis labels by 70 degrees and use a log scale for both axes.
Display the histogram using plt.show().


# Import matplotlib.pyplot
import matplotlib.pyplot as plt

# Plot the histogram
df['Existing Zoning Sqft'].plot(kind='hist', rot=70, logx=True, logy=True)

# Display the histogram
plt.show()

    
    
 ____________________________________________________________________________________________________________________
 
 Visualizing multiple variables with boxplots
Histograms are great ways of visualizing single variables. To visualize multiple variables, boxplots are useful, especially when one of the variables is categorical.

In this exercise, your job is to use a boxplot to compare the 'initial_cost' across the different values of the 'Borough' column. The pandas .boxplot() method is a quick way to do this, in which you have to specify the column and by parameters. Here, you want to visualize how 'initial_cost' varies by 'Borough'.

pandas and matplotlib.pyplot have been imported for you as pd and plt, respectively, and the DataFrame has been pre-loaded as df.

Instructions
100 XP
Using the .boxplot() method of df, create a boxplot of 'initial_cost' across the different values of 'Borough'.
Display the plot.


# Import necessary modules
import pandas as pd
import matplotlib.pyplot as plt

# Create the boxplot
df.boxplot(column='initial_cost', by='Borough', rot=90)

# Display the plot
plt.show()


___________________________________________________________________________________________________________________

Visualizing multiple variables with scatter plots
Boxplots are great when you have a numeric column that you want to compare across different categories. When you want to visualize two numeric columns, scatter plots are ideal.

In this exercise, your job is to make a scatter plot with 'initial_cost' on the x-axis and the 'total_est_fee' on the y-axis. You can do this by using the DataFrame .plot() method with kind='scatter'. You'll notice right away that there are 2 major outliers shown in the plots.

Since these outliers dominate the plot, an additional DataFrame, df_subset, has been provided, in which some of the extreme values have been removed. After making a scatter plot using this, you'll find some interesting patterns here that would not have been seen by looking at summary statistics or 1 variable plots.

When you're done, you can cycle between the two plots by clicking the 'Previous Plot' and 'Next Plot' buttons below the plot.

Instructions
100 XP
Instructions
100 XP
Using df, create a scatter plot (kind='scatter') with 'initial_cost' on the x-axis and the 'total_est_fee' on the y-axis. Rotate the x-axis labels by 70 degrees.
Create another scatter plot exactly as above, substituting df_subset in place of df.

# Import necessary modules
import pandas as pd
import matplotlib.pyplot as plt

# Create and display the first scatter plot
df.plot(kind='scatter', x='initial_cost', y='total_est_fee', rot=70)
plt.show()

# Create and display the second scatter plot
df_subset.plot(kind='scatter', x='initial_cost', y='total_est_fee', rot=70)
plt.show()


=======================================================================================================

2
Tidying data for analysis


Here, you'll learn about the principles of tidy data and more importantly, why you should care about them and how they make subsequent data analysis more efficient. You'll gain first hand experience with reshaping and tidying your data using techniques such as pivoting and melting.



Recognizing tidy data
For data to be tidy, it must have:

Each variable as a separate column.
Each row as a separate observation.
As a data scientist, you'll encounter data that is represented in a variety of different ways, so it is important to be able to recognize tidy (or untidy) data when you see it.

In this exercise, two example datasets have been pre-loaded into the DataFrames df1 and df2. Only one of them is tidy. Your job is to explore these further in the IPython Shell and identify the one that is not tidy, and why it is not tidy.

In the rest of this course, you will frequently be asked to explore the structure of DataFrames in the IPython Shell prior to performing different operations on them. Doing this will not only strengthen your comprehension of the data cleaning concepts covered in this course, but will also help you realize and take advantage of the relationship between working in the Shell and in the script.

____________________________________________________________________________________________________________________

eshaping your data using melt
Melting data is the process of turning columns of your data into rows of data. Consider the DataFrames from the previous exercise. In the tidy DataFrame, the variables Ozone, Solar.R, Wind, and Temp each had their own column. If, however, you wanted these variables to be in rows instead, you could melt the DataFrame. In doing so, however, you would make the data untidy! This is important to keep in mind: Depending on how your data is represented, you will have to reshape it differently.

In this exercise, you will practice melting a DataFrame using pd.melt(). There are two parameters you should be aware of: id_vars and value_vars. The id_vars represent the columns of the data you do not want to melt (i.e., keep it in its current shape), while the value_vars represent the columns you do wish to melt into rows. By default, if no value_vars are provided, all columns not set in the id_vars will be melted. This could save a bit of typing, depending on the number of columns that need to be melted.

The (tidy) DataFrame airquality has been pre-loaded. Your job is to melt its Ozone, Solar.R, Wind, and Temp columns into rows. Later in this chapter, you'll learn how to bring this melted DataFrame back into a tidy form.

Instructions
100 XP
Instructions
100 XP
Print the head of airquality.
Use pd.melt() to melt the Ozone, Solar.R, Wind, and Temp columns of airquality into rows. Do this by using id_vars to specify the columns you do not wish to melt: 'Month' and 'Day'.
Print the head of airquality_melt.

# Print the head of airquality
print(airquality.head())

# Melt airquality: airquality_melt
airquality_melt = pd.melt(airquality, id_vars=['Month', 'Day'])

# Print the head of airquality_melt
print(airquality_melt.head())



<script.py> output:
       Ozone  Solar.R  Wind  Temp  Month  Day
    0   41.0    190.0   7.4    67      5    1
    1   36.0    118.0   8.0    72      5    2
    2   12.0    149.0  12.6    74      5    3
    3   18.0    313.0  11.5    62      5    4
    4    NaN      NaN  14.3    56      5    5
       Month  Day variable  value
    0      5    1    Ozone   41.0
    1      5    2    Ozone   36.0
    2      5    3    Ozone   12.0
    3      5    4    Ozone   18.0
    4      5    5    Ozone    NaN
    
    
    ___________________________________________________________________________________________________________________________
    
    
    Customizing melted data
When melting DataFrames, it would be better to have column names more meaningful than variable and value.

The default names may work in certain situations, but it's best to always have data that is self explanatory.

You can rename the variable column by specifying an argument to the var_name parameter, and the value column by specifying an argument to the value_name parameter. You will now practice doing exactly this. The DataFrame airquality has been pre-loaded for you.

Instructions
100 XP
Print the head of airquality.
Melt the Ozone, Solar.R, Wind, and Temp columns of airquality into rows, with the default variable column renamed to 'measurement' and the default value column renamed to 'reading'. You can do this by specifying, respectively, the var_name and value_name parameters.
Print the head of airquality_melt.
    
    
 # Print the head of airquality
print(airquality.head())

# Melt airquality: airquality_melt
airquality_melt = pd.melt(airquality, id_vars=['Month', 'Day'], var_name='measurement', value_name='reading')

# Print the head of airquality_melt
print(airquality_melt.head())


<script.py> output:
       Ozone  Solar.R  Wind  Temp  Month  Day
    0   41.0    190.0   7.4    67      5    1
    1   36.0    118.0   8.0    72      5    2
    2   12.0    149.0  12.6    74      5    3
    3   18.0    313.0  11.5    62      5    4
    4    NaN      NaN  14.3    56      5    5
       Month  Day measurement  reading
    0      5    1       Ozone     41.0
    1      5    2       Ozone     36.0
    2      5    3       Ozone     12.0
    3      5    4       Ozone     18.0
    4      5    5       Ozone      NaN
    
    
    ________________________________________________________________________________________________________________________
    
    
    
    Pivot data
Pivoting data is the opposite of melting it. Remember the tidy form that the airquality DataFrame was in before you melted it? You'll now begin pivoting it back into that form using the .pivot_table() method!

While melting takes a set of columns and turns it into a single column, pivoting will create a new column for each unique value in a specified column.

.pivot_table() has an index parameter which you can use to specify the columns that you don't want pivoted: It is similar to the id_vars parameter of pd.melt(). Two other parameters that you have to specify are columns (the name of the column you want to pivot), and values (the values to be used when the column is pivoted). The melted DataFrame airquality_melt has been pre-loaded for you.

Instructions
100 XP
Print the head of airquality_melt.
Pivot airquality_melt by using .pivot_table() with the rows indexed by 'Month' and 'Day', the columns indexed by 'measurement', and the values populated with 'reading'.
Print the head of airquality_pivot.



# Print the head of airquality_melt
print(airquality_melt.head())

# Pivot airquality_melt: airquality_pivot
airquality_pivot = airquality_melt.pivot_table(index=['Month','Day'], columns='measurement', values='reading')

# Print the head of airquality_pivot
print(airquality_pivot.head())


<script.py> output:
       Month  Day measurement  reading
    0      5    1       Ozone     41.0
    1      5    2       Ozone     36.0
    2      5    3       Ozone     12.0
    3      5    4       Ozone     18.0
    4      5    5       Ozone      NaN
    measurement  Ozone  Solar.R  Temp  Wind
    Month Day                              
    5     1       41.0    190.0  67.0   7.4
          2       36.0    118.0  72.0   8.0
          3       12.0    149.0  74.0  12.6
          4       18.0    313.0  62.0  11.5
          5        NaN      NaN  56.0  14.3
          
          
          
 _________________________________________________________________________________________________________________________
 
 
 
 Resetting the index of a DataFrame
After pivoting airquality_melt in the previous exercise, you didn't quite get back the original DataFrame.

What you got back instead was a pandas DataFrame with a hierarchical index (also known as a MultiIndex).

Hierarchical indexes are covered in depth in Manipulating DataFrames with pandas. In essence, they allow you to group columns or rows by another variable - in this case, by 'Month' as well as 'Day'.

There's a very simple method you can use to get back the original DataFrame from the pivoted DataFrame: .reset_index(). Dan didn't show you how to use this method in the video, but you're now going to practice using it in this exercise to get back the original DataFrame from airquality_pivot, which has been pre-loaded.

Instructions
100 XP
Print the index of airquality_pivot by accessing its .index attribute. This has been done for you.
Reset the index of airquality_pivot using its .reset_index() method.
Print the new index of airquality_pivot.
Print the head of airquality_pivot.



# Print the index of airquality_pivot
print(airquality_pivot.index)

# Reset the index of airquality_pivot: airquality_pivot_reset
airquality_pivot_reset = airquality_pivot.reset_index()

# Print the new index of airquality_pivot_reset
print(airquality_pivot_reset.index)

# Print the head of airquality_pivot_reset
print(airquality_pivot_reset.head())



<script.py> output:
    MultiIndex(levels=[[5, 6, 7, 8, 9], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]],
               labels=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]],
               names=['Month', 'Day'])
    RangeIndex(start=0, stop=153, step=1)
    measurement  Month  Day  Ozone  Solar.R  Temp  Wind
    0                5    1   41.0    190.0  67.0   7.4
    1                5    2   36.0    118.0  72.0   8.0
    2                5    3   12.0    149.0  74.0  12.6
    3                5    4   18.0    313.0  62.0  11.5
    4                5    5    NaN      NaN  56.0  14.3
    
    
    
________________________________________________________________________________________________________________________

Pivoting duplicate values
So far, you've used the .pivot_table() method when there are multiple index values you want to hold constant during a pivot. In the video, Dan showed you how you can also use pivot tables to deal with duplicate values by providing an aggregation function through the aggfunc parameter. Here, you're going to combine both these uses of pivot tables.

Let's say your data collection method accidentally duplicated your dataset. Such a dataset, in which each row is duplicated, has been pre-loaded as airquality_dup. In addition, the airquality_melt DataFrame from the previous exercise has been pre-loaded. Explore their shapes in the IPython Shell by accessing their .shape attributes to confirm the duplicate rows present in airquality_dup.

You'll see that by using .pivot_table() and the aggfunc parameter, you can not only reshape your data, but also remove duplicates. Finally, you can then flatten the columns of the pivoted DataFrame using .reset_index().

NumPy and pandas have been imported as np and pd respectively.

Instructions
100 XP
Instructions
100 XP
Pivot airquality_dup by using .pivot_table() with the rows indexed by 'Month' and 'Day', the columns indexed by 'measurement', and the values populated with 'reading'. Use np.mean for the aggregation function.
Flatten airquality_pivot by resetting its index.
Print the head of airquality_pivot and then the original airquality DataFrame to compare their structure.


# Pivot airquality_dup: airquality_pivot
airquality_pivot = airquality_dup.pivot_table(index=['Month','Day'], columns='measurement', values='reading', aggfunc=np.mean)

# Reset the index of airquality_pivot
airquality_pivot = airquality_pivot.reset_index()

# Print the head of airquality_pivot
print(airquality_pivot.head())

# Print the head of airquality
print(airquality.head())



<script.py> output:
    measurement  Month  Day  Ozone  Solar.R  Temp  Wind
    0                5    1   41.0    190.0  67.0   7.4
    1                5    2   36.0    118.0  72.0   8.0
    2                5    3   12.0    149.0  74.0  12.6
    3                5    4   18.0    313.0  62.0  11.5
    4                5    5    NaN      NaN  56.0  14.3
       Ozone  Solar.R  Wind  Temp  Month  Day
    0   41.0    190.0   7.4    67      5    1
    1   36.0    118.0   8.0    72      5    2
    2   12.0    149.0  12.6    74      5    3
    3   18.0    313.0  11.5    62      5    4
    4    NaN      NaN  14.3    56      5    5
_________________________________________________________________________________________________________________________________


 
Splitting a column with .str
The dataset you saw in the video, consisting of case counts of tuberculosis by country, year, gender, and age group, has been pre-loaded into a DataFrame as tb.

In this exercise, you're going to tidy the 'm014' column, which represents males aged 0-14 years of age. In order to parse this value, you need to extract the first letter into a new column for gender, and the rest into a column for age_group. Here, since you can parse values by position, you can take advantage of pandas' vectorized string slicing by using the str attribute of columns of type object.

Begin by printing the columns of tb in the IPython Shell using its .columns attribute, and take note of the problematic column.

Instructions
100 XP
Melt tb keeping 'country' and 'year' fixed.
Create a 'gender' column by slicing the first letter of the variable column of tb_melt.
Create an 'age_group' column by slicing the rest of the variable column of tb_melt.
Print the head of tb_melt. This has been done for you, so hit 'Submit Answer' to see the results!


# Melt tb: tb_melt
tb_melt = pd.melt(tb, id_vars=['country','year'])

# Create the 'gender' column
tb_melt['gender'] = tb_melt.variable.str[0]

# Create the 'age_group' column
tb_melt['age_group'] = tb_melt.variable.str[1:]

# Print the head of tb_melt
print(tb_melt.head())


<script.py> output:
      country  year variable  value gender age_group
    0      AD  2000     m014    0.0      m       014
    1      AE  2000     m014    2.0      m       014
    2      AF  2000     m014   52.0      m       014
    3      AG  2000     m014    0.0      m       014
    4      AL  2000     m014    2.0      m       014
    
    
    ____________________________________________________________________________________________________________________________
    
    Splitting a column with .split() and .get()
Another common way multiple variables are stored in columns is with a delimiter. You'll learn how to deal with such cases in this exercise, using a dataset consisting of Ebola cases and death counts by state and country. It has been pre-loaded into a DataFrame as ebola.

Print the columns of ebola in the IPython Shell using ebola.columns. Notice that the data has column names such as Cases_Guinea and Deaths_Guinea. Here, the underscore _ serves as a delimiter between the first part (cases or deaths), and the second part (country).

This time, you cannot directly slice the variable by position as in the previous exercise. You now need to use Python's built-in string method called .split(). By default, this method will split a string into parts separated by a space. However, in this case you want it to split by an underscore. You can do this on Cases_Guinea, for example, using Cases_Guinea.split('_'), which returns the list ['Cases', 'Guinea'].

The next challenge is to extract the first element of this list and assign it to a type variable, and the second element of the list to a country variable. You can accomplish this by accessing the str attribute of the column and using the .get() method to retrieve the 0 or 1 index, depending on the part you want.

Instructions
100 XP
Instructions
100 XP
Melt ebola using 'Date' and 'Day' as the id_vars, 'type_country' as the var_name, and 'counts' as the value_name.
Create a column called 'str_split' by splitting the 'type_country' column of ebola_melt on '_'. Note that you will first have to access the str attribute of type_country before you can use .split().
Create a column called 'type' by using the .get() method to retrieve index 0 of the 'str_split' column of ebola_melt.
Create a column called 'country' by using the .get() method to retrieve index 1 of the 'str_split' column of ebola_melt.
Print the head of ebola. This has been done for you, so hit 'Submit Answer' to view the results!



# Melt ebola: ebola_melt
ebola_melt = pd.melt(ebola, id_vars=['Date', 'Day'], var_name='type_country', value_name='counts')

# Create the 'str_split' column
ebola_melt['str_split'] = ebola_melt.type_country.str.split('_')

# Create the 'type' column
ebola_melt['type'] = ebola_melt.str_split.str.get(0)

# Create the 'country' column
ebola_melt['country'] = ebola_melt.str_split.str.get(1)

# Print the head of ebola_melt
print(ebola_melt.head())


<script.py> output:
             Date  Day  type_country  counts        str_split   type country
    0    1/5/2015  289  Cases_Guinea  2776.0  [Cases, Guinea]  Cases  Guinea
    1    1/4/2015  288  Cases_Guinea  2775.0  [Cases, Guinea]  Cases  Guinea
    2    1/3/2015  287  Cases_Guinea  2769.0  [Cases, Guinea]  Cases  Guinea
    3    1/2/2015  286  Cases_Guinea     NaN  [Cases, Guinea]  Cases  Guinea
    4  12/31/2014  284  Cases_Guinea  2730.0  [Cases, Guinea]  Cases  Guinea

============================================================================================================

3
Combining data for analysis


The ability to transform and combine your data is a crucial skill in data science, because your data may not always come in one monolithic file or table for you to load. A large dataset may be broken into separate datasets to facilitate easier storage and sharing. Or if you are dealing with time series data, for example, you may have a new dataset for each day. No matter the reason, it is important to be able to combine datasets so you can either clean a single dataset, or clean each dataset separately and then combine them later so you can run your analysis on a single dataset. In this chapter, you'll learn all about combining data.



Combining rows of data
The dataset you'll be working with here relates to NYC Uber data. The original dataset has all the originating Uber pickup locations by time and latitude and longitude. For didactic purposes, you'll be working with a very small portion of the actual data.

Three DataFrames have been pre-loaded: uber1, which contains data for April 2014, uber2, which contains data for May 2014, and uber3, which contains data for June 2014. Your job in this exercise is to concatenate these DataFrames together such that the resulting DataFrame has the data for all three months.

Begin by exploring the structure of these three DataFrames in the IPython Shell using methods such as .head().

Instructions
100 XP
Concatenate uber1, uber2, and uber3 together using pd.concat(). You'll have to pass the DataFrames in as a list.
Print the shape and then the head of the concatenated DataFrame, row_concat.

# Concatenate uber1, uber2, and uber3: row_concat
row_concat = pd.concat([uber1, uber2, uber3])

# Print the shape of row_concat
print(row_concat.shape)

# Print the head of row_concat
print(row_concat.head())


<script.py> output:
    (297, 4)
              Date/Time      Lat      Lon    Base
    0  4/1/2014 0:11:00  40.7690 -73.9549  B02512
    1  4/1/2014 0:17:00  40.7267 -74.0345  B02512
    2  4/1/2014 0:21:00  40.7316 -73.9873  B02512
    3  4/1/2014 0:28:00  40.7588 -73.9776  B02512
    4  4/1/2014 0:33:00  40.7594 -73.9722  B02512
    
    
    ___________________________________________________________________________________________________________________________-
    
    
    Combining columns of data
Think of column-wise concatenation of data as stitching data together from the sides instead of the top and bottom. To perform this action, you use the same pd.concat() function, but this time with the keyword argument axis=1. The default, axis=0, is for a row-wise concatenation.

You'll return to the Ebola dataset you worked with briefly in the last chapter. It has been pre-loaded into a DataFrame called ebola_melt. In this DataFrame, the status and country of a patient is contained in a single column. This column has been parsed into a new DataFrame, status_country, where there are separate columns for status and country.

Explore the ebola_melt and status_country DataFrames in the IPython Shell. Your job is to concatenate them column-wise in order to obtain a final, clean DataFrame.

Instructions
100 XP
Concatenate ebola_melt and status_country column-wise into a single DataFrame called ebola_tidy. Be sure to specify axis=1 and to pass the two DataFrames in as a list.
Print the shape and then the head of the concatenated DataFrame, ebola_tidy.


# Concatenate ebola_melt and status_country column-wise: ebola_tidy
ebola_tidy = pd.concat([ebola_melt, status_country], axis = 1)

# Print the shape of ebola_tidy
print(ebola_tidy.shape)

# Print the head of ebola_tidy
print(ebola_tidy.head())


         Date  Day status_country  counts
0    1/5/2015  289   Cases_Guinea  2776.0
1    1/4/2015  288   Cases_Guinea  2775.0
2    1/3/2015  287   Cases_Guinea  2769.0
3    1/2/2015  286   Cases_Guinea     NaN
4  12/31/2014  284   Cases_Guinea  2730.0
  status country
0  Cases  Guinea
1  Cases  Guinea
2  Cases  Guinea
3  Cases  Guinea
4  Cases  Guinea

<script.py> output:
    (1952, 6)
             Date  Day status_country  counts status country
    0    1/5/2015  289   Cases_Guinea  2776.0  Cases  Guinea
    1    1/4/2015  288   Cases_Guinea  2775.0  Cases  Guinea
    2    1/3/2015  287   Cases_Guinea  2769.0  Cases  Guinea
    3    1/2/2015  286   Cases_Guinea     NaN  Cases  Guinea
    4  12/31/2014  284   Cases_Guinea  2730.0  Cases  Guinea
    
    
    ______________________________________________________________________________________________________________________
    
    
Finding files that match a pattern
You're now going to practice using the glob module to find all csv files in the workspace. In the next exercise, you'll programmatically load them into DataFrames.

As Dan showed you in the video, the glob module has a function called glob that takes a pattern and returns a list of the files in the working directory that match that pattern.

For example, if you know the pattern is part_ single digit number .csv, you can write the pattern as 'part_?.csv' (which would match part_1.csv, part_2.csv, part_3.csv, etc.)

Similarly, you can find all .csv files with '*.csv', or all parts with 'part_*'. The ? wildcard represents any 1 character, and the * wildcard represents any number of characters.

Instructions
100 XP
Instructions
100 XP
Import the glob module along with pandas (as its usual alias pd).
Write a pattern to match all .csv files.
Save all files that match the pattern using the glob() function within the glob module. That is, by using glob.glob().
Print the list of file names. This has been done for you.# Import necessary modules


import glob as glob
import pandas as pd

# Write the pattern: pattern
pattern = '*.csv'

# Save all file matches: csv_files
csv_files = glob.glob(pattern) 

# Print the file names
print(csv_files)

# Load the second file into a DataFrame: csv2
csv2 = pd.read_csv(csv_files[1])

# Print the head of csv2
print(csv2.head())

Read the second file in csv_files (i.e., index 1) into a DataFrame called csv2.
Hit 'Submit Answer' to print the head of csv2. Does it look familiar?





<script.py> output:
    ['uber-raw-data-2014_05.csv', 'uber-raw-data-2014_04.csv', 'uber-raw-data-2014_06.csv']
              Date/Time      Lat      Lon    Base
    0  4/1/2014 0:11:00  40.7690 -73.9549  B02512
    1  4/1/2014 0:17:00  40.7267 -74.0345  B02512
    2  4/1/2014 0:21:00  40.7316 -73.9873  B02512
    3  4/1/2014 0:28:00  40.7588 -73.9776  B02512
    4  4/1/2014 0:33:00  40.7594 -73.9722  B02512
    
    
    
    ____________________________________________________________________________________________________________________
    
    

Iterating and concatenating all matches
Now that you have a list of filenames to load, you can load all the files into a list of DataFrames that can then be concatenated.

You'll start with an empty list called frames. Your job is to use a for loop to iterate through each of the filenames, read each filename into a DataFrame, and then append it to the frames list.

You can then concatenate this list of DataFrames using pd.concat(). Go for it!

Instructions
100 XP
Write a for loop to iterate through csv_files:
In each iteration of the loop, read csv into a DataFrame called df.
After creating df, append it to the list frames using the .append() method.
Concatenate frames into a single DataFrame called uber.
Hit 'Submit Answer' to see the head and shape of the concatenated DataFrame!




# Create an empty list: frames
frames = []

#  Iterate over csv_files
for csv in csv_files:

    #  Read csv into a DataFrame: df
    df = pd.read_csv(csv)
    
    # Append df to frames
    frames.append(df)

# Concatenate frames into a single DataFrame: uber
uber = pd.concat(frames)

# Print the shape of uber
print(uber.shape)

# Print the head of uber
print(uber.head())


<script.py> output:
    (297, 4)
              Date/Time      Lat      Lon    Base
    0  5/1/2014 0:02:00  40.7521 -73.9914  B02512
    1  5/1/2014 0:06:00  40.6965 -73.9715  B02512
    2  5/1/2014 0:15:00  40.7464 -73.9838  B02512
    3  5/1/2014 0:17:00  40.7463 -74.0011  B02512
    4  5/1/2014 0:17:00  40.7594 -73.9734  B02512
    
    
    _________________________________________________________________________________________________________________
    
    
    1-to-1 data merge
Merging data allows you to combine disparate datasets into a single dataset to do more complex analysis.

Here, you'll be using survey data that contains readings that William Dyer, Frank Pabodie, and Valentina Roerich took in the late 1920 and 1930 while they were on an expedition towards Antarctica. The dataset was taken from a sqlite database from the Software Carpentry SQL lesson.

Two DataFrames have been pre-loaded: site and visited. Explore them in the IPython Shell and take note of their structure and column names. Your task is to perform a 1-to-1 merge of these two DataFrames using the 'name' column of site and the 'site' column of visited.

Instructions
100 XP
Merge the site and visited DataFrames on the 'name' column of site and 'site' column of visited.
Print the merged DataFrame o2o.




# Merge the DataFrames: o2o
o2o = pd.merge(left=site, right=visited, left_on='name', right_on='site')

# Print o2o
print(o2o)


<script.py> output:
        name    lat    long  ident   site       dated
    0   DR-1 -49.85 -128.57    619   DR-1  1927-02-08
    1   DR-3 -47.15 -126.72    734   DR-3  1939-01-07
    2  MSK-4 -48.87 -123.40    837  MSK-4  1932-01-14
    
    
 _______________________________________________________________________________________________________________________________
 
 
 
 Many-to-1 data merge
In a many-to-one (or one-to-many) merge, one of the values will be duplicated and recycled in the output. That is, one of the keys in the merge is not unique.

Here, the two DataFrames site and visited have been pre-loaded once again. Note that this time, visited has multiple entries for the site column. Confirm this by exploring it in the IPython Shell.

The .merge() method call is the same as the 1-to-1 merge from the previous exercise, but the data and output will be different.

Instructions
100 XP
Merge the site and visited DataFrames on the 'name' column of site and 'site' column of visited, exactly as you did in the previous exercise.
Print the merged DataFrame and then hit 'Submit Answer' to see the different output produced by this merge!


# Merge the DataFrames: m2o
m2o = pd.merge(left=site, right=visited, left_on='name', right_on='site')

# Print m2o
print(m2o)


<script.py> output:
        name    lat    long  ident   site       dated
    0   DR-1 -49.85 -128.57    619   DR-1  1927-02-08
    1   DR-1 -49.85 -128.57    622   DR-1  1927-02-10
    2   DR-1 -49.85 -128.57    844   DR-1  1932-03-22
    3   DR-3 -47.15 -126.72    734   DR-3  1939-01-07
    4   DR-3 -47.15 -126.72    735   DR-3  1930-01-12
    5   DR-3 -47.15 -126.72    751   DR-3  1930-02-26
    6   DR-3 -47.15 -126.72    752   DR-3         NaN
    7  MSK-4 -48.87 -123.40    837  MSK-4  1932-01-14
    
    
    ________________________________________________________________________________________________________________________________
    
    
    
    Many-to-many data merge
The final merging scenario occurs when both DataFrames do not have unique keys for a merge. What happens here is that for each duplicated key, every pairwise combination will be created.

Two example DataFrames that share common key values have been pre-loaded: df1 and df2. Another DataFrame df3, which is the result of df1 merged with df2, has been pre-loaded. All three DataFrames have been printed - look at the output and notice how pairwise combinations have been created. This example is to help you develop your intuition for many-to-many merges.

Here, you'll work with the site and visited DataFrames from before, and a new survey DataFrame. Your task is to merge site and visited as you did in the earlier exercises. You will then merge this merged DataFrame with survey.

Begin by exploring the site, visited, and survey DataFrames in the IPython Shell.

Instructions
100 XP
Instructions
100 XP
Merge the site and visited DataFrames on the 'name' column of site and 'site' column of visited, exactly as you did in the previous two exercises. Save the result as m2m.
Merge the m2m and survey DataFrames on the 'ident' column of m2m and 'taken' column of survey.
Hit 'Submit Answer' to print the first 20 lines of the merged DataFrame!


    
    
# Merge site and visited: m2m
m2m = pd.merge(left=site, right=visited, left_on='name', right_on='site')

# Merge m2m and survey: m2m
m2m = pd.merge(left=m2m, right=survey, left_on='ident', right_on='taken')

# Print the first 20 lines of m2m
print(m2m.head(20))


  c1  c2
0  a   1
1  a   2
2  b   3
3  b   4
  c1  c2
0  a  10
1  a  20
2  b  30
3  b  40
  c1  c2_x  c2_y
0  a     1    10
1  a     1    20
2  a     2    10
3  a     2    20
4  b     3    30
5  b     3    40
6  b     4    30
7  b     4    40

<script.py> output:
         name    lat    long  ident   site       dated  taken person quant  \
    0    DR-1 -49.85 -128.57    619   DR-1  1927-02-08    619   dyer   rad   
    1    DR-1 -49.85 -128.57    619   DR-1  1927-02-08    619   dyer   sal   
    2    DR-1 -49.85 -128.57    622   DR-1  1927-02-10    622   dyer   rad   
    3    DR-1 -49.85 -128.57    622   DR-1  1927-02-10    622   dyer   sal   
    4    DR-1 -49.85 -128.57    844   DR-1  1932-03-22    844    roe   rad   
    5    DR-3 -47.15 -126.72    734   DR-3  1939-01-07    734     pb   rad   
    6    DR-3 -47.15 -126.72    734   DR-3  1939-01-07    734   lake   sal   
    7    DR-3 -47.15 -126.72    734   DR-3  1939-01-07    734     pb  temp   
    8    DR-3 -47.15 -126.72    735   DR-3  1930-01-12    735     pb   rad   
    9    DR-3 -47.15 -126.72    735   DR-3  1930-01-12    735    NaN   sal   
    10   DR-3 -47.15 -126.72    735   DR-3  1930-01-12    735    NaN  temp   
    11   DR-3 -47.15 -126.72    751   DR-3  1930-02-26    751     pb   rad   
    12   DR-3 -47.15 -126.72    751   DR-3  1930-02-26    751     pb  temp   
    13   DR-3 -47.15 -126.72    751   DR-3  1930-02-26    751   lake   sal   
    14   DR-3 -47.15 -126.72    752   DR-3         NaN    752   lake   rad   
    15   DR-3 -47.15 -126.72    752   DR-3         NaN    752   lake   sal   
    16   DR-3 -47.15 -126.72    752   DR-3         NaN    752   lake  temp   
    17   DR-3 -47.15 -126.72    752   DR-3         NaN    752    roe   sal   
    18  MSK-4 -48.87 -123.40    837  MSK-4  1932-01-14    837   lake   rad   
    19  MSK-4 -48.87 -123.40    837  MSK-4  1932-01-14    837   lake   sal   
    
        reading  
    0      9.82  
    1      0.13  
    2      7.80  
    3      0.09  
    4     11.25  
    5      8.41  
    6      0.05  
    7    -21.50  
    8      7.22  
    9      0.06  
    10   -26.00  
    11     4.35  
    12   -18.50  
    13     0.10  
    14     2.19  
    15     0.09  
    16   -16.00  
    17    41.60  
    18     1.46  
    19     0.21
    
   
===============================================================================================================

4
Cleaning data for analysis


Here, you'll dive into some of the grittier aspects of data cleaning. You'll learn about string manipulation and pattern matching to deal with unstructured data, and then explore techniques to deal with missing or duplicate data. You'll also learn the valuable skill of programmatically checking your data for consistency, which will give you confidence that your code is running correctly and that the results of your analysis are reliable!


Converting data types
In this exercise, you'll see how ensuring all categorical variables in a DataFrame are of type category reduces memory usage.

The tips dataset has been loaded into a DataFrame called tips. This data contains information about how much a customer tipped, whether the customer was male or female, a smoker or not, etc.

Look at the output of tips.info() in the IPython Shell. You'll note that two columns that should be categorical - sex and smoker - are instead of type object, which is pandas' way of storing arbitrary strings. Your job is to convert these two columns to type category and note the reduced memory usage.

Instructions
100 XP
Convert the sex column of the tips DataFrame to type 'category' using the .astype() method.
Convert the smoker column of the tips DataFrame.
Print the memory usage of tips after converting the data types of the columns. Use the .info() method to do this.




# Convert the sex column to type 'category'
tips.sex = tips.sex.astype('category')

# Convert the smoker column to type 'category'
tips.smoker = tips.smoker.astype('category')

# Print the info of tips
print(tips.info())


# Convert the sex column to type 'category'
tips.sex = tips.sex.astype('category')

# Convert the smoker column to type 'category'
tips.smoker = tips.smoker.astype('category')

# Print the info of tips
print(tips.info())


<script.py> output:
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 244 entries, 0 to 243
    Data columns (total 7 columns):
    total_bill    244 non-null float64
    tip           244 non-null float64
    sex           244 non-null category
    smoker        244 non-null category
    day           244 non-null object
    time          244 non-null object
    size          244 non-null int64
    dtypes: category(2), float64(2), int64(1), object(2)
    memory usage: 10.1+ KB
    None
    
    
    
    ______________________________________________________________________________________________________________________--
    
    
    

Working with numeric data
If you expect the data type of a column to be numeric (int or float), but instead it is of type object, this typically means that there is a non numeric value in the column, which also signifies bad data.

You can use the pd.to_numeric() function to convert a column into a numeric data type. If the function raises an error, you can be sure that there is a bad value within the column. You can either use the techniques you learned in Chapter 1 to do some exploratory data analysis and find the bad value, or you can choose to ignore or coerce the value into a missing value, NaN.

A modified version of the tips dataset has been pre-loaded into a DataFrame called tips. For instructional purposes, it has been pre-processed to introduce some 'bad' data for you to clean. Use the .info() method to explore this. You'll note that the total_bill and tip columns, which should be numeric, are instead of type object. Your job is to fix this.

Instructions
100 XP
Instructions
100 XP
Use pd.to_numeric() to convert the 'total_bill' column of tips to a numeric data type. Coerce the errors to NaN by specifying the keyword argument errors='coerce'.
Convert the 'tip' column of 'tips' to a numeric data type exactly as you did for the 'total_bill' column.
Print the info of tips to confirm that the data types of 'total_bill' and 'tips' are numeric.

# Convert 'total_bill' to a numeric dtype
tips['total_bill'] = pd.to_numeric(tips['total_bill'], errors='coerce')

# Convert 'tip' to a numeric dtype
tips['tip'] = pd.to_numeric(tips['tip'], errors='coerce')

# Print the info of tips
print(tips.info())

<script.py> output:
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 244 entries, 0 to 243
    Data columns (total 7 columns):
    total_bill    202 non-null float64
    tip           220 non-null float64
    sex           234 non-null category
    smoker        229 non-null category
    day           243 non-null category
    time          227 non-null category
    size          231 non-null float64
    dtypes: category(4), float64(3)
    memory usage: 6.9 KB
    None
    
    
    
   _________________________________________________________________________________________________________________________
   
   
   String parsing with regular expressions
In the video, Dan introduced you to the basics of regular expressions, which are powerful ways of defining patterns to match strings. This exercise will get you started with writing them.

When working with data, it is sometimes necessary to write a regular expression to look for properly entered values. Phone numbers in a dataset is a common field that needs to be checked for validity. Your job in this exercise is to define a regular expression to match US phone numbers that fit the pattern of xxx-xxx-xxxx.

The regular expression module in python is re. When performing pattern matching on data, since the pattern will be used for a match across multiple rows, it's better to compile the pattern first using re.compile(), and then use the compiled pattern to match values.

Instructions
100 XP
Import re.
Compile a pattern that matches a phone number of the format xxx-xxx-xxxx.
Use \d{x} to match x digits. Here you'll need to use it three times: twice to match 3 digits, and once to match 4 digits.
Place the regular expression inside re.compile().
Using the .match() method on prog, check whether the pattern matches the string '123-456-7890'.
Using the same approach, now check whether the pattern matches the string '1123-456-7890'.


# Import the regular expression module
import re

# Compile the pattern: prog
prog = re.compile('\d{3}-\d{3}-\d{4}')

# See if the pattern matches
result = prog.match('123-456-7890')
print(bool(result))

# See if the pattern matches
result2 = prog.match('1123-456-7890')
print(bool(result2))



   ___________________________________________________________________________________________________________________
   
   Extracting numerical values from strings
Extracting numbers from strings is a common task, particularly when working with unstructured data or log files.

Say you have the following string: 'the recipe calls for 6 strawberries and 2 bananas'.

It would be useful to extract the 6 and the 2 from this string to be saved for later use when comparing strawberry to banana ratios.

When using a regular expression to extract multiple numbers (or multiple pattern matches, to be exact), you can use the re.findall() function. Dan did not discuss this in the video, but it is straightforward to use: You pass in a pattern and a string to re.findall(), and it will return a list of the matches.

Instructions
100 XP
Instructions
100 XP
Import re.
Write a pattern that will find all the numbers in the following string: 'the recipe calls for 10 strawberries and 1 banana'. To do this:
Use the re.findall() function and pass it two arguments: the pattern, followed by the string.
\d is the pattern required to find digits. This should be followed with a + so that the previous element is matched one or more times. This ensures that 10 is viewed as one number and not as 1 and 0.
Print the matches to confirm that your regular expression found the values 10 and 1.
   
   
   
   # Import the regular expression module
import re

# Find the numeric values: matches
matches = re.findall('\d+', 'the recipe calls for 10 strawberries and 1 banana')

# Print the matches
print(matches)


<script.py> output:
    ['10', '1']
    
    
    __________________________________________________________________________________________________________________
    
    Pattern matching
In this exercise, you'll continue practicing your regular expression skills. For each provided string, your job is to write the appropriate pattern to match it.

Instructions
100 XP
Write patterns to match:
A telephone number of the format xxx-xxx-xxxx. You already did this in a previous exercise.
A string of the format: A dollar sign, an arbitrary number of digits, a decimal point, 2 digits.
Use \$ to match the dollar sign, \d* to match an arbitrary number of digits, \. to match the decimal point, and \d{x} to match x number of digits.
A capital letter, followed by an arbitrary number of alphanumeric characters.
Use [A-Z] to match any capital letter followed by \w* to match an arbitrary number of alphanumeric characters.


# Write the first pattern
pattern1 = bool(re.match(pattern='\d{3}-\d{3}-\d{4}', string='123-456-7890'))
print(pattern1)

# Write the second pattern
pattern2 = bool(re.match(pattern='\$\d*\.\d{2}', string='$123.45'))
print(pattern2)

# Write the third pattern
pattern3 = bool(re.match(pattern='[A-Z]\w*', string='Australia'))
print(pattern3)

<script.py> output:
    True
    True
    True
    
 _________________________________________________________________________________________________________________________
   
   
   Custom functions to clean data
You'll now practice writing functions to clean data.

The tips dataset has been pre-loaded into a DataFrame called tips. It has a 'sex' column that contains the values 'Male' or 'Female'. Your job is to write a function that will recode 'Male' to 1, 'Female' to 0, and return np.nan for all entries of 'sex' that are neither 'Male' nor 'Female'.

Recoding variables like this is a common data cleaning task. Functions provide a mechanism for you to abstract away complex bits of code as well as reuse code. This makes your code more readable and less error prone.

As Dan showed you in the videos, you can use the .apply() method to apply a function across entire rows or columns of DataFrames. However, note that each column of a DataFrame is a pandas Series. Functions can also be applied across Series. Here, you will apply your function over the 'sex' column.

Instructions
100 XP
Instructions
100 XP
Define a function named recode_sex() that has one parameter: sex_value.
If sex_value equals 'Male', return 1.
Else, if sex_value equals 'Female', return 0.
If sex_value does not equal 'Male' or 'Female', return np.nan. NumPy has been pre-imported for you.
Apply your recode_sex() function over tips.sex using the .apply() method to create a new column: 'sex_recode'. Note that when passing in a function inside the .apply() method, you don't need to specify the parentheses after the function name.
Hit 'Submit Answer' and take note of the new 'sex_recode' column in the tips DataFrame!


# Define recode_sex()
def recode_sex(sex_value):

    # Return 1 if sex_value is 'Male'
    if sex_value == 'Male':
        return 1
    
    # Return 0 if sex_value is 'Female'    
    elif sex_value == 'Female':
        return 0
    
    # Return np.nan    
    else:
        return np.nan

# Apply the function to the sex column
tips['sex_recode'] = tips.sex.apply(recode_sex)

# Print the first five rows of tips
print(tips.head())

<script.py> output:
       total_bill   tip     sex smoker  day    time  size  sex_recode
    0       16.99  1.01  Female     No  Sun  Dinner   2.0         0.0
    1       10.34  1.66    Male     No  Sun  Dinner   3.0         1.0
    2       21.01  3.50    Male     No  Sun  Dinner   3.0         1.0
    3         NaN  3.31    Male     No  Sun  Dinner   2.0         1.0
    4         NaN  3.61  Female     No  Sun  Dinner   4.0         0.0
    
    
    
    ________________________________________________________________________________________________________________________
    
    

Lambda functions
You'll now be introduced to a powerful Python feature that will help you clean your data more effectively: lambda functions. Instead of using the def syntax that you used in the previous exercise, lambda functions let you make simple, one-line functions.

For example, here's a function that squares a variable used in an .apply() method:

def my_square(x):
    return x ** 2

df.apply(my_square)
The equivalent code using a lambda function is:

df.apply(lambda x: x ** 2)
The lambda function takes one parameter - the variable x. The function itself just squares x and returns the result, which is whatever the one line of code evaluates to. In this way, lambda functions can make your code concise and Pythonic.

The tips dataset has been pre-loaded into a DataFrame called tips. Your job is to clean its 'total_dollar' column by removing the dollar sign. You'll do this using two different methods: With the .replace() method, and with regular expressions. The regular expression module re has been pre-imported.

Instructions
100 XP
Use the .replace() method inside a lambda function to remove the dollar sign from the 'total_dollar' column of tips.
You need to specify two arguments to the .replace() method: The string to be replaced ('$'), and the string to replace it by ('').
Apply the lambda function over the 'total_dollar' column of tips.
Use a regular expression to remove the dollar sign from the 'total_dollar' column of tips.
The pattern has been provided for you: It is the first argument of the re.findall() function.
Complete the rest of the lambda function and apply it over the 'total_dollar' column of tips. Notice that because re.findall() returns a list, you have to slice it in order to access the actual value.
Hit 'Submit Answer' to verify that you have removed the dollar sign from the column.


# Write the lambda function using replace
tips['total_dollar_replace'] = tips.total_dollar.apply(lambda x: x.replace('$', ''))

# Write the lambda function using regular expressions
tips['total_dollar_re'] = tips.total_dollar.apply(lambda x: re.findall('\d+\.\d+', x)[0])

# Print the head of tips
print(tips.head())

<script.py> output:
       total_bill   tip     sex smoker  day    time  size total_dollar  \
    0       16.99  1.01  Female     No  Sun  Dinner     2       $16.99   
    1       10.34  1.66    Male     No  Sun  Dinner     3       $10.34   
    2       21.01  3.50    Male     No  Sun  Dinner     3       $21.01   
    3       23.68  3.31    Male     No  Sun  Dinner     2       $23.68   
    4       24.59  3.61  Female     No  Sun  Dinner     4       $24.59   
    
      total_dollar_replace total_dollar_re  
    0                16.99           16.99  
    1                10.34           10.34  
    2                21.01           21.01  
    3                23.68           23.68  
    4                24.59           24.59
    
    
    ____________________________________________________________________________________________________________________________
    
    


































   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   


























======================================================================================================================

5
Case study


In this final chapter, you'll apply all of the data cleaning techniques you've learned in this course towards tidying a real-world, messy dataset obtained from the Gapminder Foundation. Once you're done, not only will you have a clean and tidy dataset, you'll also be ready to start working on your own data science projects using the power of Python!

